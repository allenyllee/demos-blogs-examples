{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Convolutional Q-Learning Network\n",
    "\n",
    "**TODO**: introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.10.5'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "# plt.style.use('ggplot') # use \"ggplot\" style for graphs\n",
    "pltparams = {'legend.fontsize':14,'axes.labelsize':18,'axes.titlesize':18,\n",
    "             'xtick.labelsize':12,'ytick.labelsize':12,'figure.figsize':(7.5,7.5),}\n",
    "plt.rcParams.update(pltparams)\n",
    "%matplotlib nbagg\n",
    "\n",
    "gym.logger.set_level(40)\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mholub/miniconda3/envs/dl36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 1.8.0\n",
      "Keras version: 2.1.5\n",
      "Keras config: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'floatx': 'float32',\n",
       " 'epsilon': 1e-07,\n",
       " 'backend': 'tensorflow',\n",
       " 'image_data_format': 'channels_last'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import __version__ as K_ver\n",
    "from keras.backend import _config as K_cf\n",
    "from tensorflow import __version__ as tf_ver\n",
    "print(\"Tensorflow version: {}\".format(tf_ver))\n",
    "print(\"Keras version: {}\".format(K_ver))\n",
    "print(\"Keras config: \\n\")\n",
    "K_cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# game.epsilon = np.float32(1.)\n",
    "game.num_episodes = int(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"blah\"\n",
    "template = '{step: ' + text + 'd}/{nb_steps}: ' + 'episode: {episode}, duration: {duration:.3f}s, ' + 'episode steps: {episode_steps}, steps per second: {sps:.0f}, ' + 'episode reward: {episode_reward:.3f}, ' +'mean reward: {reward_mean:.3f} [{reward_min:.3f}, {reward_max:.3f}], ' + 'mean action: {action_mean:.3f} [{action_min:.3f}, {action_max:.3f}], ' + 'mean observation: {obs_mean:.3f} [{obs_min:.3f}, {obs_max:.3f}],' + '{metrics}, ' +'mean epsilon: {eps_mean:.3f} [{eps_min:.3f}, {eps_max:.3f}], ' + 'lr: {lr:.6f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "2018-07-09 21:16:05,539:atari_game:INFO: Environment: BreakoutDeterministic-v4. There are 4 actions: ['NOOP', 'FIRE', 'RIGHT', 'LEFT']. Frameskip: 4\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "frames (InputLayer)             (None, 80, 80, 4)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 80, 80, 4)    0           frames[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 19, 19, 16)   4112        lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 19, 19, 16)   0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 8, 8, 32)     8224        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 8, 32)     0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 2048)         0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          262272      flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 4)            516         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "mask (InputLayer)               (None, 4)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, 4)            0           dense_4[0][0]                    \n",
      "                                                                 mask[0][0]                       \n",
      "==================================================================================================\n",
      "Total params: 275,124\n",
      "Trainable params: 275,124\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Successfully constructed networks.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from atari_game import AtariGame\n",
    "\n",
    "game = AtariGame(mode = \"DQN\", num_episodes = 250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-07-07 23:05:09,304:atari_game:INFO: Initializing memory for 20000 steps\n",
      "2018-07-07 23:05:09,390:atari_game:INFO: Model trained, step: 1\n",
      "Training for 5000000 steps ...\n",
      "      46/5000000: episode: 1, duration: 3.944s, episode steps: 46, steps per second: 12, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.543 [0.000, 3.000], mean observation: 39.642 [0.000, 142.000], loss: 0.180348, mean_absolute_error: 2.261606, mean_q: 3.064485, mean epsilon: 1.000 [1.000, 1.000]\n",
      "      89/5000000: episode: 2, duration: 3.697s, episode steps: 43, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.651 [0.000, 3.000], mean observation: 40.015 [0.000, 142.000], loss: 0.195643, mean_absolute_error: 2.216665, mean_q: 2.990252, mean epsilon: 1.000 [1.000, 1.000]\n",
      "     138/5000000: episode: 3, duration: 4.429s, episode steps: 49, steps per second: 11, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 1.469 [0.000, 3.000], mean observation: 39.760 [0.000, 142.000], loss: 0.181559, mean_absolute_error: 2.238240, mean_q: 3.024808, mean epsilon: 0.997 [0.997, 0.997]\n",
      "     187/5000000: episode: 4, duration: 4.495s, episode steps: 49, steps per second: 11, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 1.327 [0.000, 3.000], mean observation: 39.659 [0.000, 142.000], loss: 0.209081, mean_absolute_error: 2.279883, mean_q: 2.999699, mean epsilon: 0.995 [0.995, 0.995]\n",
      "     268/5000000: episode: 5, duration: 6.824s, episode steps: 81, steps per second: 12, episode reward: 3.000, mean reward: 0.037 [0.000, 1.000], mean action: 1.519 [0.000, 3.000], mean observation: 39.792 [0.000, 142.000], loss: 0.187242, mean_absolute_error: 2.237867, mean_q: 3.017405, mean epsilon: 0.992 [0.992, 0.992]\n",
      "     314/5000000: episode: 6, duration: 4.277s, episode steps: 46, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.652 [0.000, 3.000], mean observation: 39.981 [0.000, 142.000], loss: 0.190075, mean_absolute_error: 2.240861, mean_q: 3.061705, mean epsilon: 0.989 [0.989, 0.989]\n",
      "     374/5000000: episode: 7, duration: 5.046s, episode steps: 60, steps per second: 12, episode reward: 2.000, mean reward: 0.033 [0.000, 1.000], mean action: 1.350 [0.000, 3.000], mean observation: 39.730 [0.000, 142.000], loss: 0.192390, mean_absolute_error: 2.246186, mean_q: 3.015477, mean epsilon: 0.987 [0.987, 0.987]\n",
      "     438/5000000: episode: 8, duration: 5.651s, episode steps: 64, steps per second: 11, episode reward: 4.000, mean reward: 0.062 [0.000, 1.000], mean action: 1.609 [0.000, 3.000], mean observation: 39.720 [0.000, 142.000], loss: 0.196621, mean_absolute_error: 2.252420, mean_q: 3.050532, mean epsilon: 0.984 [0.984, 0.984]\n",
      "     500/5000000: episode: 9, duration: 5.281s, episode steps: 62, steps per second: 12, episode reward: 3.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.565 [0.000, 3.000], mean observation: 39.699 [0.000, 142.000], loss: 0.184596, mean_absolute_error: 2.192234, mean_q: 2.980089, mean epsilon: 0.981 [0.981, 0.981]\n",
      "     532/5000000: episode: 10, duration: 2.805s, episode steps: 32, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.750 [0.000, 3.000], mean observation: 39.996 [0.000, 142.000], loss: 0.204015, mean_absolute_error: 2.312045, mean_q: 3.149911, mean epsilon: 0.978 [0.978, 0.978]\n",
      "     578/5000000: episode: 11, duration: 3.900s, episode steps: 46, steps per second: 12, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.587 [0.000, 3.000], mean observation: 39.899 [0.000, 142.000], loss: 0.202770, mean_absolute_error: 2.254207, mean_q: 3.041921, mean epsilon: 0.976 [0.976, 0.976]\n",
      "     647/5000000: episode: 12, duration: 5.766s, episode steps: 69, steps per second: 12, episode reward: 3.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.536 [0.000, 3.000], mean observation: 39.658 [0.000, 142.000], loss: 0.200330, mean_absolute_error: 2.257092, mean_q: 3.046870, mean epsilon: 0.973 [0.973, 0.973]\n",
      "     718/5000000: episode: 13, duration: 6.032s, episode steps: 71, steps per second: 12, episode reward: 3.000, mean reward: 0.042 [0.000, 1.000], mean action: 1.423 [0.000, 3.000], mean observation: 39.750 [0.000, 142.000], loss: 0.205693, mean_absolute_error: 2.290398, mean_q: 3.096648, mean epsilon: 0.970 [0.970, 0.970]\n",
      "     784/5000000: episode: 14, duration: 5.727s, episode steps: 66, steps per second: 12, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.409 [0.000, 3.000], mean observation: 39.676 [0.000, 142.000], loss: 0.185485, mean_absolute_error: 2.271413, mean_q: 3.081261, mean epsilon: 0.968 [0.968, 0.968]\n",
      "     845/5000000: episode: 15, duration: 5.361s, episode steps: 61, steps per second: 11, episode reward: 3.000, mean reward: 0.049 [0.000, 1.000], mean action: 1.672 [0.000, 3.000], mean observation: 39.701 [0.000, 142.000], loss: 0.196597, mean_absolute_error: 2.226519, mean_q: 2.990739, mean epsilon: 0.965 [0.965, 0.965]\n",
      "     881/5000000: episode: 16, duration: 3.115s, episode steps: 36, steps per second: 12, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 1.444 [0.000, 3.000], mean observation: 39.820 [0.000, 142.000], loss: 0.203046, mean_absolute_error: 2.251455, mean_q: 3.018733, mean epsilon: 0.962 [0.962, 0.962]\n",
      "2018-07-07 23:06:30,207:atari_game:INFO: Episode 15/1000 finished. Mean reward over last 15 episodes: 1.81\n",
      "     918/5000000: episode: 17, duration: 3.178s, episode steps: 37, steps per second: 12, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 1.432 [0.000, 3.000], mean observation: 39.835 [0.000, 142.000], loss: 0.195553, mean_absolute_error: 2.242043, mean_q: 3.031631, mean epsilon: 0.959 [0.960, 0.960]\n",
      "2018-07-07 23:06:34,556:atari_game:INFO: Target updated, step: 999\n",
      "    1005/5000000: episode: 18, duration: 7.528s, episode steps: 87, steps per second: 12, episode reward: 5.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.448 [0.000, 3.000], mean observation: 39.495 [0.000, 142.000], loss: 0.183276, mean_absolute_error: 2.193966, mean_q: 2.993211, mean epsilon: 0.957 [0.957, 0.957]\n",
      "    1033/5000000: episode: 19, duration: 2.566s, episode steps: 28, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.357 [0.000, 3.000], mean observation: 39.993 [0.000, 142.000], loss: 0.203372, mean_absolute_error: 2.117829, mean_q: 2.893308, mean epsilon: 0.954 [0.954, 0.954]\n",
      "    1097/5000000: episode: 20, duration: 6.631s, episode steps: 64, steps per second: 10, episode reward: 2.000, mean reward: 0.031 [0.000, 1.000], mean action: 1.719 [0.000, 3.000], mean observation: 39.767 [0.000, 142.000], loss: 0.196215, mean_absolute_error: 2.192790, mean_q: 2.999461, mean epsilon: 0.951 [0.951, 0.951]\n",
      "    1153/5000000: episode: 21, duration: 5.978s, episode steps: 56, steps per second: 9, episode reward: 1.000, mean reward: 0.018 [0.000, 1.000], mean action: 1.393 [0.000, 3.000], mean observation: 39.834 [0.000, 142.000], loss: 0.186140, mean_absolute_error: 2.155768, mean_q: 2.924377, mean epsilon: 0.949 [0.949, 0.949]\n",
      "    1228/5000000: episode: 22, duration: 6.929s, episode steps: 75, steps per second: 11, episode reward: 4.000, mean reward: 0.053 [0.000, 1.000], mean action: 1.480 [0.000, 3.000], mean observation: 39.605 [0.000, 142.000], loss: 0.190766, mean_absolute_error: 2.161924, mean_q: 2.973586, mean epsilon: 0.946 [0.946, 0.946]\n",
      "    1278/5000000: episode: 23, duration: 4.666s, episode steps: 50, steps per second: 11, episode reward: 2.000, mean reward: 0.040 [0.000, 1.000], mean action: 1.380 [0.000, 3.000], mean observation: 39.707 [0.000, 142.000], loss: 0.194083, mean_absolute_error: 2.154241, mean_q: 2.907653, mean epsilon: 0.943 [0.943, 0.943]\n",
      "    1325/5000000: episode: 24, duration: 4.331s, episode steps: 47, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.681 [0.000, 3.000], mean observation: 39.825 [0.000, 142.000], loss: 0.190482, mean_absolute_error: 2.138601, mean_q: 2.916252, mean epsilon: 0.941 [0.941, 0.941]\n",
      "    1399/5000000: episode: 25, duration: 7.405s, episode steps: 74, steps per second: 10, episode reward: 3.000, mean reward: 0.041 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 39.652 [0.000, 142.000], loss: 0.194761, mean_absolute_error: 2.141566, mean_q: 2.932115, mean epsilon: 0.938 [0.938, 0.938]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1448/5000000: episode: 26, duration: 4.386s, episode steps: 49, steps per second: 11, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 1.327 [0.000, 3.000], mean observation: 39.705 [0.000, 142.000], loss: 0.202553, mean_absolute_error: 2.195606, mean_q: 3.020206, mean epsilon: 0.935 [0.935, 0.935]\n",
      "    1492/5000000: episode: 27, duration: 3.816s, episode steps: 44, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.545 [0.000, 3.000], mean observation: 40.009 [0.000, 142.000], loss: 0.209144, mean_absolute_error: 2.137787, mean_q: 2.918083, mean epsilon: 0.932 [0.932, 0.932]\n",
      "    1543/5000000: episode: 28, duration: 4.418s, episode steps: 51, steps per second: 12, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 1.490 [0.000, 3.000], mean observation: 39.794 [0.000, 142.000], loss: 0.201398, mean_absolute_error: 2.174440, mean_q: 2.942765, mean epsilon: 0.930 [0.930, 0.930]\n",
      "    1618/5000000: episode: 29, duration: 6.788s, episode steps: 75, steps per second: 11, episode reward: 4.000, mean reward: 0.053 [0.000, 1.000], mean action: 1.387 [0.000, 3.000], mean observation: 39.639 [0.000, 142.000], loss: 0.184686, mean_absolute_error: 2.151613, mean_q: 2.933680, mean epsilon: 0.927 [0.927, 0.927]\n",
      "    1663/5000000: episode: 30, duration: 4.307s, episode steps: 45, steps per second: 10, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.800 [0.000, 3.000], mean observation: 39.646 [0.000, 142.000], loss: 0.194228, mean_absolute_error: 2.193561, mean_q: 2.981120, mean epsilon: 0.924 [0.924, 0.924]\n",
      "    1726/5000000: episode: 31, duration: 5.959s, episode steps: 63, steps per second: 11, episode reward: 3.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.286 [0.000, 3.000], mean observation: 39.668 [0.000, 142.000], loss: 0.188895, mean_absolute_error: 2.162258, mean_q: 2.962893, mean epsilon: 0.922 [0.922, 0.922]\n",
      "2018-07-07 23:07:50,615:atari_game:INFO: Episode 30/1000 finished. Mean reward over last 15 episodes: 2.00\n",
      "    1763/5000000: episode: 32, duration: 3.615s, episode steps: 37, steps per second: 10, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.378 [0.000, 3.000], mean observation: 39.976 [0.000, 142.000], loss: 0.192673, mean_absolute_error: 2.140835, mean_q: 2.895817, mean epsilon: 0.919 [0.919, 0.919]\n",
      "    1816/5000000: episode: 33, duration: 4.910s, episode steps: 53, steps per second: 11, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.755 [0.000, 3.000], mean observation: 39.642 [0.000, 142.000], loss: 0.187535, mean_absolute_error: 2.153133, mean_q: 2.955884, mean epsilon: 0.916 [0.916, 0.916]\n",
      "    1868/5000000: episode: 34, duration: 4.991s, episode steps: 52, steps per second: 10, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 1.462 [0.000, 3.000], mean observation: 39.847 [0.000, 142.000], loss: 0.198542, mean_absolute_error: 2.165986, mean_q: 2.989207, mean epsilon: 0.914 [0.914, 0.914]\n",
      "    1918/5000000: episode: 35, duration: 4.583s, episode steps: 50, steps per second: 11, episode reward: 2.000, mean reward: 0.040 [0.000, 1.000], mean action: 1.440 [0.000, 3.000], mean observation: 39.741 [0.000, 142.000], loss: 0.187084, mean_absolute_error: 2.169932, mean_q: 2.966772, mean epsilon: 0.911 [0.911, 0.911]\n",
      "2018-07-07 23:08:11,276:atari_game:INFO: Target updated, step: 1999\n",
      "    1967/5000000: episode: 36, duration: 4.542s, episode steps: 49, steps per second: 11, episode reward: 3.000, mean reward: 0.061 [0.000, 1.000], mean action: 1.429 [0.000, 3.000], mean observation: 39.768 [0.000, 142.000], loss: 0.195495, mean_absolute_error: 2.151622, mean_q: 2.919167, mean epsilon: 0.908 [0.908, 0.908]\n",
      "    2038/5000000: episode: 37, duration: 6.525s, episode steps: 71, steps per second: 11, episode reward: 4.000, mean reward: 0.056 [0.000, 1.000], mean action: 1.563 [0.000, 3.000], mean observation: 39.645 [0.000, 142.000], loss: 0.200021, mean_absolute_error: 2.145697, mean_q: 2.878755, mean epsilon: 0.906 [0.905, 0.905]\n",
      "    2110/5000000: episode: 38, duration: 6.627s, episode steps: 72, steps per second: 11, episode reward: 4.000, mean reward: 0.056 [0.000, 1.000], mean action: 1.611 [0.000, 3.000], mean observation: 39.549 [0.000, 142.000], loss: 0.187033, mean_absolute_error: 2.137268, mean_q: 2.886719, mean epsilon: 0.903 [0.903, 0.903]\n",
      "    2194/5000000: episode: 39, duration: 7.698s, episode steps: 84, steps per second: 11, episode reward: 5.000, mean reward: 0.060 [0.000, 1.000], mean action: 1.631 [0.000, 3.000], mean observation: 39.549 [0.000, 142.000], loss: 0.194437, mean_absolute_error: 2.150928, mean_q: 2.889892, mean epsilon: 0.900 [0.900, 0.900]\n",
      "    2278/5000000: episode: 40, duration: 8.244s, episode steps: 84, steps per second: 10, episode reward: 5.000, mean reward: 0.060 [0.000, 1.000], mean action: 1.440 [0.000, 3.000], mean observation: 39.610 [0.000, 142.000], loss: 0.186796, mean_absolute_error: 2.132235, mean_q: 2.865531, mean epsilon: 0.897 [0.897, 0.897]\n",
      "    2350/5000000: episode: 41, duration: 6.257s, episode steps: 72, steps per second: 12, episode reward: 5.000, mean reward: 0.069 [0.000, 1.000], mean action: 1.625 [0.000, 3.000], mean observation: 39.467 [0.000, 142.000], loss: 0.189502, mean_absolute_error: 2.150894, mean_q: 2.904848, mean epsilon: 0.895 [0.895, 0.895]\n",
      "    2385/5000000: episode: 42, duration: 3.038s, episode steps: 35, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.543 [0.000, 3.000], mean observation: 40.049 [0.000, 142.000], loss: 0.191862, mean_absolute_error: 2.148977, mean_q: 2.913513, mean epsilon: 0.892 [0.892, 0.892]\n",
      "    2436/5000000: episode: 43, duration: 4.431s, episode steps: 51, steps per second: 12, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 1.431 [0.000, 3.000], mean observation: 39.767 [0.000, 142.000], loss: 0.188752, mean_absolute_error: 2.157178, mean_q: 2.929922, mean epsilon: 0.889 [0.889, 0.889]\n",
      "    2493/5000000: episode: 44, duration: 4.967s, episode steps: 57, steps per second: 11, episode reward: 3.000, mean reward: 0.053 [0.000, 1.000], mean action: 1.614 [0.000, 3.000], mean observation: 39.719 [0.000, 142.000], loss: 0.185268, mean_absolute_error: 2.130988, mean_q: 2.901277, mean epsilon: 0.887 [0.887, 0.887]\n",
      "    2538/5000000: episode: 45, duration: 3.898s, episode steps: 45, steps per second: 12, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 1.711 [0.000, 3.000], mean observation: 39.704 [0.000, 142.000], loss: 0.202551, mean_absolute_error: 2.151376, mean_q: 2.894860, mean epsilon: 0.884 [0.884, 0.884]\n",
      "    2577/5000000: episode: 46, duration: 3.377s, episode steps: 39, steps per second: 12, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.487 [0.000, 3.000], mean observation: 39.912 [0.000, 142.000], loss: 0.185546, mean_absolute_error: 2.174586, mean_q: 2.955880, mean epsilon: 0.881 [0.881, 0.881]\n",
      "2018-07-07 23:09:09,849:atari_game:INFO: Episode 45/1000 finished. Mean reward over last 15 episodes: 2.75\n",
      "    2639/5000000: episode: 47, duration: 5.371s, episode steps: 62, steps per second: 12, episode reward: 3.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.484 [0.000, 3.000], mean observation: 39.751 [0.000, 142.000], loss: 0.199147, mean_absolute_error: 2.161529, mean_q: 2.924186, mean epsilon: 0.879 [0.878, 0.878]\n",
      "    2705/5000000: episode: 48, duration: 6.111s, episode steps: 66, steps per second: 11, episode reward: 2.000, mean reward: 0.030 [0.000, 1.000], mean action: 1.894 [0.000, 3.000], mean observation: 39.806 [0.000, 142.000], loss: 0.188427, mean_absolute_error: 2.152179, mean_q: 2.926960, mean epsilon: 0.876 [0.876, 0.876]\n",
      "    2775/5000000: episode: 49, duration: 6.101s, episode steps: 70, steps per second: 11, episode reward: 4.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.357 [0.000, 3.000], mean observation: 39.546 [0.000, 142.000], loss: 0.191323, mean_absolute_error: 2.156217, mean_q: 2.886872, mean epsilon: 0.873 [0.873, 0.873]\n",
      "    2840/5000000: episode: 50, duration: 5.651s, episode steps: 65, steps per second: 12, episode reward: 3.000, mean reward: 0.046 [0.000, 1.000], mean action: 1.723 [0.000, 3.000], mean observation: 39.709 [0.000, 142.000], loss: 0.193600, mean_absolute_error: 2.138514, mean_q: 2.889858, mean epsilon: 0.870 [0.870, 0.870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2887/5000000: episode: 51, duration: 4.088s, episode steps: 47, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.638 [0.000, 3.000], mean observation: 39.703 [0.000, 142.000], loss: 0.189211, mean_absolute_error: 2.170507, mean_q: 2.928528, mean epsilon: 0.868 [0.868, 0.868]\n",
      "    2944/5000000: episode: 52, duration: 4.987s, episode steps: 57, steps per second: 11, episode reward: 1.000, mean reward: 0.018 [0.000, 1.000], mean action: 1.456 [0.000, 3.000], mean observation: 39.915 [0.000, 142.000], loss: 0.190516, mean_absolute_error: 2.108341, mean_q: 2.830550, mean epsilon: 0.865 [0.865, 0.865]\n",
      "2018-07-07 23:09:43,447:atari_game:INFO: Target updated, step: 2999\n",
      "    3016/5000000: episode: 53, duration: 6.261s, episode steps: 72, steps per second: 12, episode reward: 5.000, mean reward: 0.069 [0.000, 1.000], mean action: 1.319 [0.000, 3.000], mean observation: 39.489 [0.000, 142.000], loss: 0.192648, mean_absolute_error: 2.203711, mean_q: 3.014528, mean epsilon: 0.862 [0.862, 0.862]\n",
      "    3058/5000000: episode: 54, duration: 4.087s, episode steps: 42, steps per second: 10, episode reward: 1.000, mean reward: 0.024 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 39.940 [0.000, 142.000], loss: 0.204881, mean_absolute_error: 2.276683, mean_q: 3.103189, mean epsilon: 0.860 [0.860, 0.860]\n",
      "    3098/5000000: episode: 55, duration: 3.919s, episode steps: 40, steps per second: 10, episode reward: 1.000, mean reward: 0.025 [0.000, 1.000], mean action: 1.375 [0.000, 3.000], mean observation: 39.852 [0.000, 142.000], loss: 0.195651, mean_absolute_error: 2.228204, mean_q: 3.005122, mean epsilon: 0.857 [0.857, 0.857]\n",
      "    3146/5000000: episode: 56, duration: 4.192s, episode steps: 48, steps per second: 11, episode reward: 2.000, mean reward: 0.042 [0.000, 1.000], mean action: 1.604 [0.000, 3.000], mean observation: 39.751 [0.000, 142.000], loss: 0.200807, mean_absolute_error: 2.217058, mean_q: 2.989316, mean epsilon: 0.854 [0.854, 0.854]\n",
      "    3191/5000000: episode: 57, duration: 3.918s, episode steps: 45, steps per second: 11, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 1.533 [0.000, 3.000], mean observation: 39.723 [0.000, 142.000], loss: 0.188741, mean_absolute_error: 2.298754, mean_q: 3.143482, mean epsilon: 0.852 [0.851, 0.851]\n",
      "    3221/5000000: episode: 58, duration: 2.616s, episode steps: 30, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.367 [0.000, 3.000], mean observation: 40.038 [0.000, 142.000], loss: 0.186381, mean_absolute_error: 2.269345, mean_q: 3.094463, mean epsilon: 0.849 [0.849, 0.849]\n",
      "    3267/5000000: episode: 59, duration: 3.998s, episode steps: 46, steps per second: 12, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.435 [0.000, 3.000], mean observation: 39.940 [0.000, 142.000], loss: 0.193513, mean_absolute_error: 2.233677, mean_q: 3.070714, mean epsilon: 0.846 [0.846, 0.846]\n",
      "    3341/5000000: episode: 60, duration: 6.447s, episode steps: 74, steps per second: 11, episode reward: 3.000, mean reward: 0.041 [0.000, 1.000], mean action: 1.581 [0.000, 3.000], mean observation: 39.670 [0.000, 142.000], loss: 0.195858, mean_absolute_error: 2.263041, mean_q: 3.060830, mean epsilon: 0.843 [0.843, 0.843]\n",
      "    3384/5000000: episode: 61, duration: 3.777s, episode steps: 43, steps per second: 11, episode reward: 1.000, mean reward: 0.023 [0.000, 1.000], mean action: 1.674 [0.000, 3.000], mean observation: 39.881 [0.000, 142.000], loss: 0.185271, mean_absolute_error: 2.246006, mean_q: 3.049755, mean epsilon: 0.841 [0.841, 0.841]\n",
      "2018-07-07 23:10:22,883:atari_game:INFO: Episode 60/1000 finished. Mean reward over last 15 episodes: 2.00\n",
      "    3442/5000000: episode: 62, duration: 5.040s, episode steps: 58, steps per second: 12, episode reward: 2.000, mean reward: 0.034 [0.000, 1.000], mean action: 1.552 [0.000, 3.000], mean observation: 39.752 [0.000, 142.000], loss: 0.191274, mean_absolute_error: 2.240598, mean_q: 3.059102, mean epsilon: 0.838 [0.838, 0.838]\n",
      "    3489/5000000: episode: 63, duration: 4.092s, episode steps: 47, steps per second: 11, episode reward: 1.000, mean reward: 0.021 [0.000, 1.000], mean action: 1.766 [0.000, 3.000], mean observation: 39.701 [0.000, 142.000], loss: 0.214108, mean_absolute_error: 2.215597, mean_q: 3.013163, mean epsilon: 0.835 [0.835, 0.835]\n",
      "    3566/5000000: episode: 64, duration: 6.708s, episode steps: 77, steps per second: 11, episode reward: 3.000, mean reward: 0.039 [0.000, 1.000], mean action: 1.636 [0.000, 3.000], mean observation: 39.538 [0.000, 142.000], loss: 0.195156, mean_absolute_error: 2.258950, mean_q: 3.073266, mean epsilon: 0.833 [0.833, 0.833]\n",
      "    3638/5000000: episode: 65, duration: 6.266s, episode steps: 72, steps per second: 11, episode reward: 1.000, mean reward: 0.014 [0.000, 1.000], mean action: 1.792 [0.000, 3.000], mean observation: 39.970 [0.000, 142.000], loss: 0.187236, mean_absolute_error: 2.248542, mean_q: 3.085463, mean epsilon: 0.830 [0.830, 0.830]\n",
      "    3713/5000000: episode: 66, duration: 6.537s, episode steps: 75, steps per second: 11, episode reward: 3.000, mean reward: 0.040 [0.000, 1.000], mean action: 1.360 [0.000, 3.000], mean observation: 39.728 [0.000, 142.000], loss: 0.205116, mean_absolute_error: 2.248643, mean_q: 3.025036, mean epsilon: 0.827 [0.827, 0.827]\n",
      "    3765/5000000: episode: 67, duration: 4.530s, episode steps: 52, steps per second: 11, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 1.269 [0.000, 3.000], mean observation: 39.681 [0.000, 142.000], loss: 0.191916, mean_absolute_error: 2.218873, mean_q: 3.014108, mean epsilon: 0.825 [0.825, 0.825]\n",
      "    3825/5000000: episode: 68, duration: 5.299s, episode steps: 60, steps per second: 11, episode reward: 2.000, mean reward: 0.033 [0.000, 1.000], mean action: 1.600 [0.000, 3.000], mean observation: 39.718 [0.000, 142.000], loss: 0.204248, mean_absolute_error: 2.228134, mean_q: 3.005581, mean epsilon: 0.822 [0.822, 0.822]\n",
      "    3902/5000000: episode: 69, duration: 6.713s, episode steps: 77, steps per second: 11, episode reward: 2.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.818 [0.000, 3.000], mean observation: 39.844 [0.000, 142.000], loss: 0.191089, mean_absolute_error: 2.242993, mean_q: 3.064044, mean epsilon: 0.819 [0.819, 0.819]\n",
      "2018-07-07 23:11:14,114:atari_game:INFO: Target updated, step: 3999\n",
      "    3987/5000000: episode: 70, duration: 7.461s, episode steps: 85, steps per second: 11, episode reward: 6.000, mean reward: 0.071 [0.000, 1.000], mean action: 1.576 [0.000, 3.000], mean observation: 39.421 [0.000, 142.000], loss: 0.195039, mean_absolute_error: 2.244383, mean_q: 3.064037, mean epsilon: 0.816 [0.816, 0.816]\n",
      "    4022/5000000: episode: 71, duration: 3.152s, episode steps: 35, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.486 [0.000, 3.000], mean observation: 39.988 [0.000, 142.000], loss: 0.187796, mean_absolute_error: 2.329613, mean_q: 3.134228, mean epsilon: 0.814 [0.814, 0.814]\n",
      "    4084/5000000: episode: 72, duration: 5.465s, episode steps: 62, steps per second: 11, episode reward: 3.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.726 [0.000, 3.000], mean observation: 39.718 [0.000, 142.000], loss: 0.204364, mean_absolute_error: 2.340381, mean_q: 3.118970, mean epsilon: 0.811 [0.811, 0.811]\n",
      "    4144/5000000: episode: 73, duration: 5.240s, episode steps: 60, steps per second: 11, episode reward: 2.000, mean reward: 0.033 [0.000, 1.000], mean action: 1.700 [0.000, 3.000], mean observation: 39.861 [0.000, 142.000], loss: 0.202792, mean_absolute_error: 2.348939, mean_q: 3.191519, mean epsilon: 0.808 [0.808, 0.808]\n",
      "    4210/5000000: episode: 74, duration: 5.790s, episode steps: 66, steps per second: 11, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.758 [0.000, 3.000], mean observation: 39.743 [0.000, 142.000], loss: 0.193117, mean_absolute_error: 2.384898, mean_q: 3.212905, mean epsilon: 0.806 [0.806, 0.806]\n",
      "    4251/5000000: episode: 75, duration: 3.579s, episode steps: 41, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.805 [0.000, 3.000], mean observation: 39.997 [0.000, 142.000], loss: 0.211641, mean_absolute_error: 2.336512, mean_q: 3.180898, mean epsilon: 0.803 [0.803, 0.803]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    4288/5000000: episode: 76, duration: 3.250s, episode steps: 37, steps per second: 11, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 1.297 [0.000, 3.000], mean observation: 39.808 [0.000, 142.000], loss: 0.217525, mean_absolute_error: 2.346836, mean_q: 3.178336, mean epsilon: 0.800 [0.800, 0.800]\n",
      "2018-07-07 23:11:43,455:atari_game:INFO: Episode 75/1000 finished. Mean reward over last 15 episodes: 2.00\n",
      "    4356/5000000: episode: 77, duration: 5.947s, episode steps: 68, steps per second: 11, episode reward: 2.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.647 [0.000, 3.000], mean observation: 39.756 [0.000, 142.000], loss: 0.201445, mean_absolute_error: 2.328377, mean_q: 3.123583, mean epsilon: 0.798 [0.797, 0.797]\n",
      "    4395/5000000: episode: 78, duration: 3.402s, episode steps: 39, steps per second: 11, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.692 [0.000, 3.000], mean observation: 39.940 [0.000, 142.000], loss: 0.177156, mean_absolute_error: 2.363528, mean_q: 3.189764, mean epsilon: 0.795 [0.795, 0.795]\n",
      "    4437/5000000: episode: 79, duration: 3.701s, episode steps: 42, steps per second: 11, episode reward: 1.000, mean reward: 0.024 [0.000, 1.000], mean action: 1.738 [0.000, 3.000], mean observation: 39.923 [0.000, 142.000], loss: 0.203682, mean_absolute_error: 2.347110, mean_q: 3.171987, mean epsilon: 0.792 [0.792, 0.792]\n",
      "    4494/5000000: episode: 80, duration: 5.209s, episode steps: 57, steps per second: 11, episode reward: 3.000, mean reward: 0.053 [0.000, 1.000], mean action: 1.684 [0.000, 3.000], mean observation: 39.702 [0.000, 142.000], loss: 0.210253, mean_absolute_error: 2.348895, mean_q: 3.121331, mean epsilon: 0.789 [0.789, 0.789]\n",
      "    4549/5000000: episode: 81, duration: 4.927s, episode steps: 55, steps per second: 11, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 1.582 [0.000, 3.000], mean observation: 39.681 [0.000, 142.000], loss: 0.214172, mean_absolute_error: 2.379909, mean_q: 3.171033, mean epsilon: 0.787 [0.787, 0.787]\n",
      "    4602/5000000: episode: 82, duration: 4.646s, episode steps: 53, steps per second: 11, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 1.849 [0.000, 3.000], mean observation: 39.735 [0.000, 142.000], loss: 0.204527, mean_absolute_error: 2.368289, mean_q: 3.178643, mean epsilon: 0.784 [0.784, 0.784]\n",
      "    4652/5000000: episode: 83, duration: 4.358s, episode steps: 50, steps per second: 11, episode reward: 2.000, mean reward: 0.040 [0.000, 1.000], mean action: 1.700 [0.000, 3.000], mean observation: 39.724 [0.000, 142.000], loss: 0.200804, mean_absolute_error: 2.314298, mean_q: 3.113028, mean epsilon: 0.781 [0.781, 0.781]\n",
      "    4731/5000000: episode: 84, duration: 6.964s, episode steps: 79, steps per second: 11, episode reward: 4.000, mean reward: 0.051 [0.000, 1.000], mean action: 1.709 [0.000, 3.000], mean observation: 39.673 [0.000, 142.000], loss: 0.205000, mean_absolute_error: 2.336075, mean_q: 3.140005, mean epsilon: 0.779 [0.779, 0.779]\n",
      "    4787/5000000: episode: 85, duration: 4.913s, episode steps: 56, steps per second: 11, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 1.429 [0.000, 3.000], mean observation: 39.759 [0.000, 142.000], loss: 0.206015, mean_absolute_error: 2.348747, mean_q: 3.166327, mean epsilon: 0.776 [0.776, 0.776]\n",
      "    4856/5000000: episode: 86, duration: 6.050s, episode steps: 69, steps per second: 11, episode reward: 3.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.797 [0.000, 3.000], mean observation: 39.698 [0.000, 142.000], loss: 0.199187, mean_absolute_error: 2.342145, mean_q: 3.139616, mean epsilon: 0.773 [0.773, 0.773]\n",
      "    4884/5000000: episode: 87, duration: 2.482s, episode steps: 28, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.464 [0.000, 3.000], mean observation: 40.041 [0.000, 142.000], loss: 0.214598, mean_absolute_error: 2.347546, mean_q: 3.177151, mean epsilon: 0.770 [0.770, 0.770]\n",
      "    4927/5000000: episode: 88, duration: 3.752s, episode steps: 43, steps per second: 11, episode reward: 1.000, mean reward: 0.023 [0.000, 1.000], mean action: 1.209 [0.000, 3.000], mean observation: 39.788 [0.000, 142.000], loss: 0.209540, mean_absolute_error: 2.348904, mean_q: 3.179098, mean epsilon: 0.768 [0.768, 0.768]\n",
      "2018-07-07 23:12:45,008:atari_game:INFO: Target updated, step: 4999\n",
      "    5049/5000000: episode: 89, duration: 10.739s, episode steps: 122, steps per second: 11, episode reward: 7.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.877 [0.000, 3.000], mean observation: 39.242 [0.000, 142.000], loss: 0.199720, mean_absolute_error: 2.282549, mean_q: 3.061941, mean epsilon: 0.765 [0.765, 0.765]\n",
      "    5118/5000000: episode: 90, duration: 8.123s, episode steps: 69, steps per second: 8, episode reward: 3.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.899 [0.000, 3.000], mean observation: 39.785 [0.000, 142.000], loss: 0.188709, mean_absolute_error: 2.255054, mean_q: 3.031862, mean epsilon: 0.762 [0.762, 0.762]\n",
      "    5176/5000000: episode: 91, duration: 5.668s, episode steps: 58, steps per second: 10, episode reward: 2.000, mean reward: 0.034 [0.000, 1.000], mean action: 1.741 [0.000, 3.000], mean observation: 39.736 [0.000, 142.000], loss: 0.196308, mean_absolute_error: 2.207885, mean_q: 2.949986, mean epsilon: 0.760 [0.760, 0.760]\n",
      "2018-07-07 23:13:05,869:atari_game:INFO: Episode 90/1000 finished. Mean reward over last 15 episodes: 2.25\n",
      "    5248/5000000: episode: 92, duration: 6.211s, episode steps: 72, steps per second: 12, episode reward: 5.000, mean reward: 0.069 [0.000, 1.000], mean action: 1.375 [0.000, 3.000], mean observation: 39.527 [0.000, 142.000], loss: 0.194195, mean_absolute_error: 2.257113, mean_q: 3.013470, mean epsilon: 0.757 [0.757, 0.757]\n",
      "    5278/5000000: episode: 93, duration: 2.588s, episode steps: 30, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.433 [0.000, 3.000], mean observation: 39.984 [0.000, 142.000], loss: 0.186560, mean_absolute_error: 2.274640, mean_q: 3.100152, mean epsilon: 0.754 [0.754, 0.754]\n",
      "    5352/5000000: episode: 94, duration: 6.483s, episode steps: 74, steps per second: 11, episode reward: 4.000, mean reward: 0.054 [0.000, 1.000], mean action: 1.770 [0.000, 3.000], mean observation: 39.638 [0.000, 142.000], loss: 0.186416, mean_absolute_error: 2.235553, mean_q: 2.957147, mean epsilon: 0.752 [0.752, 0.752]\n",
      "    5397/5000000: episode: 95, duration: 3.864s, episode steps: 45, steps per second: 12, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 1.422 [0.000, 3.000], mean observation: 39.725 [0.000, 142.000], loss: 0.201215, mean_absolute_error: 2.241424, mean_q: 2.998320, mean epsilon: 0.749 [0.749, 0.749]\n",
      "    5466/5000000: episode: 96, duration: 5.931s, episode steps: 69, steps per second: 12, episode reward: 4.000, mean reward: 0.058 [0.000, 1.000], mean action: 1.696 [0.000, 3.000], mean observation: 39.618 [0.000, 142.000], loss: 0.200927, mean_absolute_error: 2.246721, mean_q: 2.968953, mean epsilon: 0.746 [0.746, 0.746]\n",
      "    5511/5000000: episode: 97, duration: 3.967s, episode steps: 45, steps per second: 11, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 1.333 [0.000, 3.000], mean observation: 39.771 [0.000, 142.000], loss: 0.191851, mean_absolute_error: 2.225649, mean_q: 2.974724, mean epsilon: 0.743 [0.743, 0.743]\n",
      "    5566/5000000: episode: 98, duration: 4.897s, episode steps: 55, steps per second: 11, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 1.691 [0.000, 3.000], mean observation: 39.695 [0.000, 142.000], loss: 0.193373, mean_absolute_error: 2.228930, mean_q: 2.999438, mean epsilon: 0.741 [0.741, 0.741]\n",
      "    5628/5000000: episode: 99, duration: 5.387s, episode steps: 62, steps per second: 12, episode reward: 3.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.919 [0.000, 3.000], mean observation: 39.683 [0.000, 142.000], loss: 0.204366, mean_absolute_error: 2.263107, mean_q: 2.995084, mean epsilon: 0.738 [0.738, 0.738]\n",
      "    5695/5000000: episode: 100, duration: 5.763s, episode steps: 67, steps per second: 12, episode reward: 7.000, mean reward: 0.104 [0.000, 4.000], mean action: 1.925 [0.000, 3.000], mean observation: 39.584 [0.000, 142.000], loss: 0.191842, mean_absolute_error: 2.258297, mean_q: 3.036553, mean epsilon: 0.735 [0.735, 0.735]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    5736/5000000: episode: 101, duration: 3.614s, episode steps: 41, steps per second: 11, episode reward: 1.000, mean reward: 0.024 [0.000, 1.000], mean action: 1.683 [0.000, 3.000], mean observation: 39.690 [0.000, 142.000], loss: 0.178112, mean_absolute_error: 2.258208, mean_q: 3.063965, mean epsilon: 0.733 [0.733, 0.733]\n",
      "    5799/5000000: episode: 102, duration: 5.614s, episode steps: 63, steps per second: 11, episode reward: 2.000, mean reward: 0.032 [0.000, 1.000], mean action: 1.587 [0.000, 3.000], mean observation: 39.708 [0.000, 142.000], loss: 0.191277, mean_absolute_error: 2.241627, mean_q: 2.998457, mean epsilon: 0.730 [0.730, 0.730]\n",
      "    5835/5000000: episode: 103, duration: 3.128s, episode steps: 36, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.694 [0.000, 3.000], mean observation: 40.022 [0.000, 142.000], loss: 0.214257, mean_absolute_error: 2.250681, mean_q: 2.982653, mean epsilon: 0.727 [0.727, 0.727]\n",
      "    5880/5000000: episode: 104, duration: 3.948s, episode steps: 45, steps per second: 11, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.711 [0.000, 3.000], mean observation: 39.858 [0.000, 142.000], loss: 0.194911, mean_absolute_error: 2.213914, mean_q: 2.956197, mean epsilon: 0.725 [0.725, 0.725]\n",
      "    5937/5000000: episode: 105, duration: 4.960s, episode steps: 57, steps per second: 11, episode reward: 2.000, mean reward: 0.035 [0.000, 1.000], mean action: 1.754 [0.000, 3.000], mean observation: 39.844 [0.000, 142.000], loss: 0.188798, mean_absolute_error: 2.235626, mean_q: 3.015424, mean epsilon: 0.722 [0.722, 0.722]\n",
      "2018-07-07 23:14:17,428:atari_game:INFO: Target updated, step: 5999\n",
      "    5997/5000000: episode: 106, duration: 5.275s, episode steps: 60, steps per second: 11, episode reward: 3.000, mean reward: 0.050 [0.000, 1.000], mean action: 1.650 [0.000, 3.000], mean observation: 39.728 [0.000, 142.000], loss: 0.201443, mean_absolute_error: 2.223002, mean_q: 2.951007, mean epsilon: 0.719 [0.719, 0.719]\n",
      "2018-07-07 23:14:18,996:atari_game:INFO: Episode 105/1000 finished. Mean reward over last 15 episodes: 2.50\n",
      "    6084/5000000: episode: 107, duration: 7.633s, episode steps: 87, steps per second: 11, episode reward: 12.000, mean reward: 0.138 [0.000, 4.000], mean action: 1.460 [0.000, 3.000], mean observation: 39.432 [0.000, 142.000], loss: 0.190439, mean_absolute_error: 2.182760, mean_q: 2.916276, mean epsilon: 0.716 [0.716, 0.716]\n",
      "    6137/5000000: episode: 108, duration: 4.568s, episode steps: 53, steps per second: 12, episode reward: 1.000, mean reward: 0.019 [0.000, 1.000], mean action: 1.377 [0.000, 3.000], mean observation: 39.827 [0.000, 142.000], loss: 0.197946, mean_absolute_error: 2.165800, mean_q: 2.891518, mean epsilon: 0.714 [0.714, 0.714]\n",
      "    6190/5000000: episode: 109, duration: 4.576s, episode steps: 53, steps per second: 12, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.906 [0.000, 3.000], mean observation: 39.779 [0.000, 142.000], loss: 0.193415, mean_absolute_error: 2.159640, mean_q: 2.910559, mean epsilon: 0.711 [0.711, 0.711]\n",
      "    6283/5000000: episode: 110, duration: 8.278s, episode steps: 93, steps per second: 11, episode reward: 4.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.484 [0.000, 3.000], mean observation: 39.627 [0.000, 142.000], loss: 0.188098, mean_absolute_error: 2.200236, mean_q: 2.956952, mean epsilon: 0.708 [0.708, 0.708]\n",
      "    6330/5000000: episode: 111, duration: 4.098s, episode steps: 47, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.511 [0.000, 3.000], mean observation: 39.715 [0.000, 142.000], loss: 0.176754, mean_absolute_error: 2.156073, mean_q: 2.896907, mean epsilon: 0.706 [0.706, 0.706]\n",
      "    6403/5000000: episode: 112, duration: 6.396s, episode steps: 73, steps per second: 11, episode reward: 7.000, mean reward: 0.096 [0.000, 4.000], mean action: 1.726 [0.000, 3.000], mean observation: 39.638 [0.000, 142.000], loss: 0.193026, mean_absolute_error: 2.189049, mean_q: 2.951764, mean epsilon: 0.703 [0.703, 0.703]\n",
      "    6469/5000000: episode: 113, duration: 5.714s, episode steps: 66, steps per second: 12, episode reward: 4.000, mean reward: 0.061 [0.000, 1.000], mean action: 1.288 [0.000, 3.000], mean observation: 39.671 [0.000, 142.000], loss: 0.195145, mean_absolute_error: 2.188530, mean_q: 2.899147, mean epsilon: 0.700 [0.700, 0.700]\n",
      "    6551/5000000: episode: 114, duration: 7.432s, episode steps: 82, steps per second: 11, episode reward: 4.000, mean reward: 0.049 [0.000, 1.000], mean action: 1.512 [0.000, 3.000], mean observation: 39.740 [0.000, 142.000], loss: 0.192974, mean_absolute_error: 2.158175, mean_q: 2.883301, mean epsilon: 0.698 [0.698, 0.698]\n",
      "    6596/5000000: episode: 115, duration: 3.878s, episode steps: 45, steps per second: 12, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.556 [0.000, 3.000], mean observation: 39.752 [0.000, 142.000], loss: 0.184031, mean_absolute_error: 2.230653, mean_q: 2.997070, mean epsilon: 0.695 [0.695, 0.695]\n",
      "    6661/5000000: episode: 116, duration: 5.690s, episode steps: 65, steps per second: 11, episode reward: 2.000, mean reward: 0.031 [0.000, 1.000], mean action: 1.415 [0.000, 3.000], mean observation: 39.782 [0.000, 142.000], loss: 0.187986, mean_absolute_error: 2.193977, mean_q: 2.940008, mean epsilon: 0.692 [0.692, 0.692]\n",
      "    6699/5000000: episode: 117, duration: 3.462s, episode steps: 38, steps per second: 11, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.316 [0.000, 3.000], mean observation: 39.933 [0.000, 142.000], loss: 0.189053, mean_absolute_error: 2.161307, mean_q: 2.916812, mean epsilon: 0.689 [0.690, 0.690]\n",
      "    6734/5000000: episode: 118, duration: 3.084s, episode steps: 35, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.886 [0.000, 3.000], mean observation: 39.922 [0.000, 142.000], loss: 0.186414, mean_absolute_error: 2.182165, mean_q: 2.921963, mean epsilon: 0.687 [0.687, 0.687]\n",
      "    6795/5000000: episode: 119, duration: 5.293s, episode steps: 61, steps per second: 12, episode reward: 3.000, mean reward: 0.049 [0.000, 1.000], mean action: 1.361 [0.000, 3.000], mean observation: 39.713 [0.000, 142.000], loss: 0.187916, mean_absolute_error: 2.208876, mean_q: 2.978814, mean epsilon: 0.684 [0.684, 0.684]\n",
      "    6864/5000000: episode: 120, duration: 5.904s, episode steps: 69, steps per second: 12, episode reward: 3.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.739 [0.000, 3.000], mean observation: 39.660 [0.000, 142.000], loss: 0.192155, mean_absolute_error: 2.159180, mean_q: 2.879571, mean epsilon: 0.681 [0.681, 0.681]\n",
      "    6921/5000000: episode: 121, duration: 4.935s, episode steps: 57, steps per second: 12, episode reward: 3.000, mean reward: 0.053 [0.000, 1.000], mean action: 1.737 [0.000, 3.000], mean observation: 39.766 [0.000, 142.000], loss: 0.201416, mean_absolute_error: 2.164837, mean_q: 2.891518, mean epsilon: 0.679 [0.679, 0.679]\n",
      "2018-07-07 23:15:41,408:atari_game:INFO: Episode 120/1000 finished. Mean reward over last 15 episodes: 3.38\n",
      "    6986/5000000: episode: 122, duration: 5.801s, episode steps: 65, steps per second: 11, episode reward: 3.000, mean reward: 0.046 [0.000, 1.000], mean action: 1.554 [0.000, 3.000], mean observation: 39.734 [0.000, 142.000], loss: 0.197421, mean_absolute_error: 2.154804, mean_q: 2.873375, mean epsilon: 0.676 [0.676, 0.676]\n",
      "2018-07-07 23:15:47,509:atari_game:INFO: Target updated, step: 6999\n",
      "    7034/5000000: episode: 123, duration: 4.265s, episode steps: 48, steps per second: 11, episode reward: 2.000, mean reward: 0.042 [0.000, 1.000], mean action: 1.979 [0.000, 3.000], mean observation: 39.771 [0.000, 142.000], loss: 0.186904, mean_absolute_error: 2.261093, mean_q: 3.099396, mean epsilon: 0.673 [0.673, 0.673]\n",
      "    7066/5000000: episode: 124, duration: 2.863s, episode steps: 32, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.562 [0.000, 3.000], mean observation: 40.010 [0.000, 142.000], loss: 0.190327, mean_absolute_error: 2.243745, mean_q: 3.081974, mean epsilon: 0.671 [0.671, 0.671]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    7132/5000000: episode: 125, duration: 5.781s, episode steps: 66, steps per second: 11, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.606 [0.000, 3.000], mean observation: 39.755 [0.000, 142.000], loss: 0.204105, mean_absolute_error: 2.223463, mean_q: 3.043891, mean epsilon: 0.668 [0.668, 0.668]\n",
      "    7193/5000000: episode: 126, duration: 5.313s, episode steps: 61, steps per second: 11, episode reward: 3.000, mean reward: 0.049 [0.000, 1.000], mean action: 1.639 [0.000, 3.000], mean observation: 39.694 [0.000, 142.000], loss: 0.192462, mean_absolute_error: 2.233483, mean_q: 3.060182, mean epsilon: 0.665 [0.665, 0.665]\n",
      "    7257/5000000: episode: 127, duration: 5.512s, episode steps: 64, steps per second: 12, episode reward: 4.000, mean reward: 0.062 [0.000, 1.000], mean action: 1.828 [0.000, 3.000], mean observation: 39.615 [0.000, 142.000], loss: 0.198868, mean_absolute_error: 2.247843, mean_q: 3.108390, mean epsilon: 0.662 [0.662, 0.662]\n",
      "    7287/5000000: episode: 128, duration: 2.612s, episode steps: 30, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.567 [0.000, 3.000], mean observation: 40.040 [0.000, 142.000], loss: 0.202415, mean_absolute_error: 2.240865, mean_q: 3.049510, mean epsilon: 0.660 [0.660, 0.660]\n",
      "    7322/5000000: episode: 129, duration: 3.146s, episode steps: 35, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.514 [0.000, 3.000], mean observation: 39.966 [0.000, 142.000], loss: 0.215193, mean_absolute_error: 2.208471, mean_q: 2.987286, mean epsilon: 0.657 [0.657, 0.657]\n",
      "    7389/5000000: episode: 130, duration: 5.893s, episode steps: 67, steps per second: 11, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.925 [0.000, 3.000], mean observation: 39.780 [0.000, 142.000], loss: 0.199628, mean_absolute_error: 2.202337, mean_q: 3.002358, mean epsilon: 0.654 [0.654, 0.654]\n",
      "    7427/5000000: episode: 131, duration: 3.270s, episode steps: 38, steps per second: 12, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 2.079 [0.000, 3.000], mean observation: 39.940 [0.000, 142.000], loss: 0.203070, mean_absolute_error: 2.278803, mean_q: 3.095616, mean epsilon: 0.652 [0.652, 0.652]\n",
      "    7483/5000000: episode: 132, duration: 4.919s, episode steps: 56, steps per second: 11, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 1.911 [0.000, 3.000], mean observation: 39.822 [0.000, 142.000], loss: 0.198390, mean_absolute_error: 2.237159, mean_q: 3.048894, mean epsilon: 0.649 [0.649, 0.649]\n",
      "    7532/5000000: episode: 133, duration: 4.354s, episode steps: 49, steps per second: 11, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 1.796 [0.000, 3.000], mean observation: 39.965 [0.000, 142.000], loss: 0.202145, mean_absolute_error: 2.244017, mean_q: 3.057909, mean epsilon: 0.646 [0.646, 0.646]\n",
      "    7601/5000000: episode: 134, duration: 6.012s, episode steps: 69, steps per second: 11, episode reward: 2.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.739 [0.000, 3.000], mean observation: 39.804 [0.000, 142.000], loss: 0.202857, mean_absolute_error: 2.227294, mean_q: 3.066978, mean epsilon: 0.644 [0.644, 0.644]\n",
      "    7668/5000000: episode: 135, duration: 5.861s, episode steps: 67, steps per second: 11, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.552 [0.000, 3.000], mean observation: 39.678 [0.000, 142.000], loss: 0.225242, mean_absolute_error: 2.203451, mean_q: 2.984734, mean epsilon: 0.641 [0.641, 0.641]\n",
      "    7737/5000000: episode: 136, duration: 6.053s, episode steps: 69, steps per second: 11, episode reward: 3.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.739 [0.000, 3.000], mean observation: 39.660 [0.000, 142.000], loss: 0.205929, mean_absolute_error: 2.213487, mean_q: 3.028011, mean epsilon: 0.638 [0.638, 0.638]\n",
      "2018-07-07 23:16:54,546:atari_game:INFO: Episode 135/1000 finished. Mean reward over last 15 episodes: 2.06\n",
      "    7763/5000000: episode: 137, duration: 2.255s, episode steps: 26, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.462 [0.000, 3.000], mean observation: 40.003 [0.000, 142.000], loss: 0.191346, mean_absolute_error: 2.192466, mean_q: 2.992248, mean epsilon: 0.635 [0.635, 0.635]\n",
      "    7818/5000000: episode: 138, duration: 4.730s, episode steps: 55, steps per second: 12, episode reward: 3.000, mean reward: 0.055 [0.000, 1.000], mean action: 1.527 [0.000, 3.000], mean observation: 39.686 [0.000, 142.000], loss: 0.197466, mean_absolute_error: 2.249540, mean_q: 3.067183, mean epsilon: 0.633 [0.633, 0.633]\n",
      "    7884/5000000: episode: 139, duration: 5.708s, episode steps: 66, steps per second: 12, episode reward: 7.000, mean reward: 0.106 [0.000, 4.000], mean action: 1.742 [0.000, 3.000], mean observation: 39.621 [0.000, 142.000], loss: 0.199723, mean_absolute_error: 2.257791, mean_q: 3.073178, mean epsilon: 0.630 [0.630, 0.630]\n",
      "    7958/5000000: episode: 140, duration: 6.398s, episode steps: 74, steps per second: 12, episode reward: 4.000, mean reward: 0.054 [0.000, 1.000], mean action: 1.703 [0.000, 3.000], mean observation: 39.660 [0.000, 142.000], loss: 0.198377, mean_absolute_error: 2.223226, mean_q: 3.032420, mean epsilon: 0.627 [0.627, 0.627]\n",
      "2018-07-07 23:17:17,661:atari_game:INFO: Target updated, step: 7999\n",
      "    8008/5000000: episode: 141, duration: 4.529s, episode steps: 50, steps per second: 11, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 1.980 [0.000, 3.000], mean observation: 39.959 [0.000, 142.000], loss: 0.196970, mean_absolute_error: 2.265198, mean_q: 3.050108, mean epsilon: 0.625 [0.625, 0.625]\n",
      "    8090/5000000: episode: 142, duration: 7.123s, episode steps: 82, steps per second: 12, episode reward: 3.000, mean reward: 0.037 [0.000, 1.000], mean action: 2.037 [0.000, 3.000], mean observation: 39.688 [0.000, 142.000], loss: 0.187738, mean_absolute_error: 2.259617, mean_q: 3.031660, mean epsilon: 0.622 [0.622, 0.622]\n",
      "    8150/5000000: episode: 143, duration: 5.259s, episode steps: 60, steps per second: 11, episode reward: 3.000, mean reward: 0.050 [0.000, 1.000], mean action: 1.767 [0.000, 3.000], mean observation: 39.659 [0.000, 142.000], loss: 0.203095, mean_absolute_error: 2.250492, mean_q: 3.028676, mean epsilon: 0.619 [0.619, 0.619]\n",
      "    8212/5000000: episode: 144, duration: 5.521s, episode steps: 62, steps per second: 11, episode reward: 7.000, mean reward: 0.113 [0.000, 4.000], mean action: 1.806 [0.000, 3.000], mean observation: 39.652 [0.000, 142.000], loss: 0.190077, mean_absolute_error: 2.280080, mean_q: 3.081602, mean epsilon: 0.617 [0.617, 0.617]\n",
      "    8287/5000000: episode: 145, duration: 6.546s, episode steps: 75, steps per second: 11, episode reward: 7.000, mean reward: 0.093 [0.000, 4.000], mean action: 1.880 [0.000, 3.000], mean observation: 39.623 [0.000, 142.000], loss: 0.198154, mean_absolute_error: 2.265311, mean_q: 3.021311, mean epsilon: 0.614 [0.614, 0.614]\n",
      "    8347/5000000: episode: 146, duration: 5.238s, episode steps: 60, steps per second: 11, episode reward: 3.000, mean reward: 0.050 [0.000, 1.000], mean action: 1.667 [0.000, 3.000], mean observation: 39.741 [0.000, 142.000], loss: 0.205752, mean_absolute_error: 2.263919, mean_q: 3.035693, mean epsilon: 0.611 [0.611, 0.611]\n",
      "    8414/5000000: episode: 147, duration: 5.880s, episode steps: 67, steps per second: 11, episode reward: 4.000, mean reward: 0.060 [0.000, 1.000], mean action: 1.776 [0.000, 3.000], mean observation: 39.578 [0.000, 142.000], loss: 0.204953, mean_absolute_error: 2.218551, mean_q: 3.000234, mean epsilon: 0.609 [0.609, 0.609]\n",
      "    8476/5000000: episode: 148, duration: 5.442s, episode steps: 62, steps per second: 11, episode reward: 3.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.903 [0.000, 3.000], mean observation: 39.751 [0.000, 142.000], loss: 0.188505, mean_absolute_error: 2.248253, mean_q: 3.044896, mean epsilon: 0.606 [0.606, 0.606]\n",
      "    8516/5000000: episode: 149, duration: 3.495s, episode steps: 40, steps per second: 11, episode reward: 1.000, mean reward: 0.025 [0.000, 1.000], mean action: 1.700 [0.000, 3.000], mean observation: 39.906 [0.000, 142.000], loss: 0.206254, mean_absolute_error: 2.298261, mean_q: 3.073298, mean epsilon: 0.603 [0.603, 0.603]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    8554/5000000: episode: 150, duration: 3.328s, episode steps: 38, steps per second: 11, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.526 [0.000, 3.000], mean observation: 39.714 [0.000, 142.000], loss: 0.194488, mean_absolute_error: 2.281325, mean_q: 3.065240, mean epsilon: 0.600 [0.600, 0.600]\n",
      "    8589/5000000: episode: 151, duration: 3.077s, episode steps: 35, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.657 [0.000, 3.000], mean observation: 39.724 [0.000, 142.000], loss: 0.216166, mean_absolute_error: 2.265787, mean_q: 3.087853, mean epsilon: 0.598 [0.598, 0.598]\n",
      "2018-07-07 23:18:10,591:atari_game:INFO: Episode 150/1000 finished. Mean reward over last 15 episodes: 3.19\n",
      "    8635/5000000: episode: 152, duration: 4.001s, episode steps: 46, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.761 [0.000, 3.000], mean observation: 39.746 [0.000, 142.000], loss: 0.208513, mean_absolute_error: 2.273374, mean_q: 3.051298, mean epsilon: 0.595 [0.595, 0.595]\n",
      "    8677/5000000: episode: 153, duration: 3.694s, episode steps: 42, steps per second: 11, episode reward: 2.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.619 [0.000, 3.000], mean observation: 39.764 [0.000, 142.000], loss: 0.194606, mean_absolute_error: 2.318287, mean_q: 3.126402, mean epsilon: 0.592 [0.592, 0.592]\n",
      "    8724/5000000: episode: 154, duration: 4.114s, episode steps: 47, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.723 [0.000, 3.000], mean observation: 39.975 [0.000, 142.000], loss: 0.204613, mean_absolute_error: 2.293302, mean_q: 3.073130, mean epsilon: 0.590 [0.590, 0.590]\n",
      "    8806/5000000: episode: 155, duration: 7.118s, episode steps: 82, steps per second: 12, episode reward: 3.000, mean reward: 0.037 [0.000, 1.000], mean action: 1.817 [0.000, 3.000], mean observation: 39.741 [0.000, 142.000], loss: 0.198235, mean_absolute_error: 2.285192, mean_q: 3.074771, mean epsilon: 0.587 [0.587, 0.587]\n",
      "    8845/5000000: episode: 156, duration: 3.376s, episode steps: 39, steps per second: 12, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.564 [0.000, 3.000], mean observation: 39.740 [0.000, 142.000], loss: 0.200108, mean_absolute_error: 2.265503, mean_q: 3.016593, mean epsilon: 0.584 [0.584, 0.584]\n",
      "    8877/5000000: episode: 157, duration: 2.745s, episode steps: 32, steps per second: 12, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 1.969 [0.000, 3.000], mean observation: 39.933 [0.000, 142.000], loss: 0.202960, mean_absolute_error: 2.247984, mean_q: 3.026589, mean epsilon: 0.582 [0.582, 0.582]\n",
      "    8937/5000000: episode: 158, duration: 5.190s, episode steps: 60, steps per second: 12, episode reward: 3.000, mean reward: 0.050 [0.000, 1.000], mean action: 1.483 [0.000, 3.000], mean observation: 39.656 [0.000, 142.000], loss: 0.200492, mean_absolute_error: 2.269362, mean_q: 3.064762, mean epsilon: 0.579 [0.579, 0.579]\n",
      "    8971/5000000: episode: 159, duration: 2.916s, episode steps: 34, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.265 [0.000, 3.000], mean observation: 40.015 [0.000, 142.000], loss: 0.215875, mean_absolute_error: 2.245369, mean_q: 3.029244, mean epsilon: 0.576 [0.576, 0.576]\n",
      "2018-07-07 23:18:47,878:atari_game:INFO: Target updated, step: 8999\n",
      "    9023/5000000: episode: 160, duration: 4.535s, episode steps: 52, steps per second: 11, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 1.769 [0.000, 3.000], mean observation: 39.856 [0.000, 142.000], loss: 0.206034, mean_absolute_error: 2.286653, mean_q: 3.069131, mean epsilon: 0.573 [0.573, 0.573]\n",
      "    9115/5000000: episode: 161, duration: 8.193s, episode steps: 92, steps per second: 11, episode reward: 6.000, mean reward: 0.065 [0.000, 1.000], mean action: 1.663 [0.000, 3.000], mean observation: 39.485 [0.000, 142.000], loss: 0.199482, mean_absolute_error: 2.347719, mean_q: 3.181617, mean epsilon: 0.571 [0.571, 0.571]\n",
      "    9172/5000000: episode: 162, duration: 5.112s, episode steps: 57, steps per second: 11, episode reward: 7.000, mean reward: 0.123 [0.000, 4.000], mean action: 1.930 [0.000, 3.000], mean observation: 39.639 [0.000, 142.000], loss: 0.198398, mean_absolute_error: 2.296473, mean_q: 3.097754, mean epsilon: 0.568 [0.568, 0.568]\n",
      "    9221/5000000: episode: 163, duration: 4.301s, episode steps: 49, steps per second: 11, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 1.673 [0.000, 3.000], mean observation: 39.790 [0.000, 142.000], loss: 0.215269, mean_absolute_error: 2.302567, mean_q: 3.131407, mean epsilon: 0.565 [0.565, 0.565]\n",
      "    9262/5000000: episode: 164, duration: 3.563s, episode steps: 41, steps per second: 12, episode reward: 2.000, mean reward: 0.049 [0.000, 1.000], mean action: 2.220 [0.000, 3.000], mean observation: 39.843 [0.000, 142.000], loss: 0.203512, mean_absolute_error: 2.311634, mean_q: 3.141868, mean epsilon: 0.563 [0.563, 0.563]\n",
      "    9320/5000000: episode: 165, duration: 5.085s, episode steps: 58, steps per second: 11, episode reward: 3.000, mean reward: 0.052 [0.000, 1.000], mean action: 1.466 [0.000, 3.000], mean observation: 39.625 [0.000, 142.000], loss: 0.203169, mean_absolute_error: 2.312939, mean_q: 3.120393, mean epsilon: 0.560 [0.560, 0.560]\n",
      "    9378/5000000: episode: 166, duration: 5.192s, episode steps: 58, steps per second: 11, episode reward: 3.000, mean reward: 0.052 [0.000, 1.000], mean action: 1.983 [0.000, 3.000], mean observation: 39.735 [0.000, 142.000], loss: 0.189785, mean_absolute_error: 2.340080, mean_q: 3.185827, mean epsilon: 0.557 [0.557, 0.557]\n",
      "2018-07-07 23:19:21,253:atari_game:INFO: Episode 165/1000 finished. Mean reward over last 15 episodes: 2.38\n",
      "    9440/5000000: episode: 167, duration: 5.419s, episode steps: 62, steps per second: 11, episode reward: 3.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.516 [0.000, 3.000], mean observation: 39.681 [0.000, 142.000], loss: 0.208680, mean_absolute_error: 2.300660, mean_q: 3.102419, mean epsilon: 0.554 [0.554, 0.554]\n",
      "    9524/5000000: episode: 168, duration: 7.337s, episode steps: 84, steps per second: 11, episode reward: 5.000, mean reward: 0.060 [0.000, 1.000], mean action: 1.821 [0.000, 3.000], mean observation: 39.554 [0.000, 142.000], loss: 0.214208, mean_absolute_error: 2.317644, mean_q: 3.139969, mean epsilon: 0.552 [0.552, 0.552]\n",
      "    9590/5000000: episode: 169, duration: 5.728s, episode steps: 66, steps per second: 12, episode reward: 4.000, mean reward: 0.061 [0.000, 1.000], mean action: 1.576 [0.000, 3.000], mean observation: 39.658 [0.000, 142.000], loss: 0.216102, mean_absolute_error: 2.335274, mean_q: 3.119137, mean epsilon: 0.549 [0.549, 0.549]\n",
      "    9643/5000000: episode: 170, duration: 4.702s, episode steps: 53, steps per second: 11, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.604 [0.000, 3.000], mean observation: 39.683 [0.000, 142.000], loss: 0.211743, mean_absolute_error: 2.297600, mean_q: 3.067074, mean epsilon: 0.546 [0.546, 0.546]\n",
      "    9696/5000000: episode: 171, duration: 4.703s, episode steps: 53, steps per second: 11, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.774 [0.000, 3.000], mean observation: 39.700 [0.000, 142.000], loss: 0.210293, mean_absolute_error: 2.279197, mean_q: 3.110785, mean epsilon: 0.544 [0.544, 0.544]\n",
      "    9736/5000000: episode: 172, duration: 3.584s, episode steps: 40, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.800 [0.000, 3.000], mean observation: 40.041 [0.000, 142.000], loss: 0.190337, mean_absolute_error: 2.281479, mean_q: 3.079017, mean epsilon: 0.541 [0.541, 0.541]\n",
      "    9817/5000000: episode: 173, duration: 7.119s, episode steps: 81, steps per second: 11, episode reward: 5.000, mean reward: 0.062 [0.000, 1.000], mean action: 1.691 [0.000, 3.000], mean observation: 39.618 [0.000, 142.000], loss: 0.206188, mean_absolute_error: 2.294820, mean_q: 3.094258, mean epsilon: 0.538 [0.538, 0.538]\n",
      "    9872/5000000: episode: 174, duration: 4.836s, episode steps: 55, steps per second: 11, episode reward: 2.000, mean reward: 0.036 [0.000, 1.000], mean action: 1.782 [0.000, 3.000], mean observation: 39.812 [0.000, 142.000], loss: 0.217688, mean_absolute_error: 2.324791, mean_q: 3.147894, mean epsilon: 0.536 [0.536, 0.536]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    9905/5000000: episode: 175, duration: 3.010s, episode steps: 33, steps per second: 11, episode reward: 1.000, mean reward: 0.030 [0.000, 1.000], mean action: 1.576 [0.000, 3.000], mean observation: 39.880 [0.000, 142.000], loss: 0.209439, mean_absolute_error: 2.327686, mean_q: 3.147174, mean epsilon: 0.533 [0.533, 0.533]\n",
      "    9947/5000000: episode: 176, duration: 3.811s, episode steps: 42, steps per second: 11, episode reward: 2.000, mean reward: 0.048 [0.000, 1.000], mean action: 1.786 [0.000, 3.000], mean observation: 39.835 [0.000, 142.000], loss: 0.211826, mean_absolute_error: 2.308557, mean_q: 3.140185, mean epsilon: 0.530 [0.530, 0.530]\n",
      "    9997/5000000: episode: 177, duration: 4.384s, episode steps: 50, steps per second: 11, episode reward: 3.000, mean reward: 0.060 [0.000, 1.000], mean action: 1.640 [0.000, 3.000], mean observation: 39.676 [0.000, 142.000], loss: 0.194843, mean_absolute_error: 2.327790, mean_q: 3.146798, mean epsilon: 0.527 [0.528, 0.528]\n",
      "2018-07-07 23:20:18,847:atari_game:INFO: Target updated, step: 9999\n",
      "   10057/5000000: episode: 178, duration: 5.183s, episode steps: 60, steps per second: 12, episode reward: 3.000, mean reward: 0.050 [0.000, 1.000], mean action: 1.883 [0.000, 3.000], mean observation: 39.717 [0.000, 142.000], loss: 0.201405, mean_absolute_error: 2.288991, mean_q: 3.103402, mean epsilon: 0.525 [0.525, 0.525]\n",
      "   10104/5000000: episode: 179, duration: 4.090s, episode steps: 47, steps per second: 11, episode reward: 1.000, mean reward: 0.021 [0.000, 1.000], mean action: 2.234 [0.000, 3.000], mean observation: 39.950 [0.000, 142.000], loss: 0.215186, mean_absolute_error: 2.287870, mean_q: 3.052435, mean epsilon: 0.522 [0.522, 0.522]\n",
      "   10148/5000000: episode: 180, duration: 3.953s, episode steps: 44, steps per second: 11, episode reward: 2.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.773 [0.000, 3.000], mean observation: 39.808 [0.000, 142.000], loss: 0.198216, mean_absolute_error: 2.273395, mean_q: 3.057911, mean epsilon: 0.519 [0.519, 0.519]\n",
      "   10194/5000000: episode: 181, duration: 4.045s, episode steps: 46, steps per second: 11, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.848 [0.000, 3.000], mean observation: 39.950 [0.000, 142.000], loss: 0.208211, mean_absolute_error: 2.226290, mean_q: 3.010263, mean epsilon: 0.517 [0.517, 0.517]\n",
      "2018-07-07 23:20:34,662:atari_game:INFO: Episode 180/1000 finished. Mean reward over last 15 episodes: 2.56\n",
      "   10243/5000000: episode: 182, duration: 4.236s, episode steps: 49, steps per second: 12, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 2.082 [0.000, 3.000], mean observation: 39.705 [0.000, 142.000], loss: 0.192687, mean_absolute_error: 2.290588, mean_q: 3.102772, mean epsilon: 0.514 [0.514, 0.514]\n",
      "   10289/5000000: episode: 183, duration: 3.949s, episode steps: 46, steps per second: 12, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.870 [0.000, 3.000], mean observation: 39.946 [0.000, 142.000], loss: 0.198132, mean_absolute_error: 2.260309, mean_q: 3.079877, mean epsilon: 0.511 [0.511, 0.511]\n",
      "   10336/5000000: episode: 184, duration: 4.122s, episode steps: 47, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.723 [0.000, 3.000], mean observation: 39.786 [0.000, 142.000], loss: 0.205446, mean_absolute_error: 2.268058, mean_q: 3.023702, mean epsilon: 0.509 [0.509, 0.509]\n",
      "   10393/5000000: episode: 185, duration: 5.086s, episode steps: 57, steps per second: 11, episode reward: 3.000, mean reward: 0.053 [0.000, 1.000], mean action: 1.684 [0.000, 3.000], mean observation: 39.694 [0.000, 142.000], loss: 0.201323, mean_absolute_error: 2.285183, mean_q: 3.050607, mean epsilon: 0.506 [0.506, 0.506]\n",
      "   10450/5000000: episode: 186, duration: 4.979s, episode steps: 57, steps per second: 11, episode reward: 3.000, mean reward: 0.053 [0.000, 1.000], mean action: 2.105 [0.000, 3.000], mean observation: 39.801 [0.000, 142.000], loss: 0.201904, mean_absolute_error: 2.250577, mean_q: 3.023874, mean epsilon: 0.503 [0.503, 0.503]\n",
      "   10484/5000000: episode: 187, duration: 3.003s, episode steps: 34, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.412 [0.000, 3.000], mean observation: 39.932 [0.000, 142.000], loss: 0.198048, mean_absolute_error: 2.222206, mean_q: 3.006484, mean epsilon: 0.500 [0.501, 0.501]\n",
      "   10520/5000000: episode: 188, duration: 3.148s, episode steps: 36, steps per second: 11, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 1.528 [0.000, 3.000], mean observation: 39.939 [0.000, 142.000], loss: 0.213019, mean_absolute_error: 2.276283, mean_q: 3.071349, mean epsilon: 0.498 [0.498, 0.498]\n",
      "   10572/5000000: episode: 189, duration: 4.570s, episode steps: 52, steps per second: 11, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 1.462 [0.000, 3.000], mean observation: 39.798 [0.000, 142.000], loss: 0.209327, mean_absolute_error: 2.255813, mean_q: 3.040865, mean epsilon: 0.495 [0.495, 0.495]\n",
      "   10610/5000000: episode: 190, duration: 3.320s, episode steps: 38, steps per second: 11, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.579 [0.000, 3.000], mean observation: 39.705 [0.000, 142.000], loss: 0.198648, mean_absolute_error: 2.259950, mean_q: 3.027213, mean epsilon: 0.492 [0.492, 0.492]\n",
      "   10653/5000000: episode: 191, duration: 3.761s, episode steps: 43, steps per second: 11, episode reward: 1.000, mean reward: 0.023 [0.000, 1.000], mean action: 1.837 [0.000, 3.000], mean observation: 39.864 [0.000, 142.000], loss: 0.223011, mean_absolute_error: 2.219269, mean_q: 2.970091, mean epsilon: 0.490 [0.490, 0.490]\n",
      "   10698/5000000: episode: 192, duration: 3.988s, episode steps: 45, steps per second: 11, episode reward: 2.000, mean reward: 0.044 [0.000, 1.000], mean action: 1.911 [0.000, 3.000], mean observation: 39.788 [0.000, 142.000], loss: 0.215560, mean_absolute_error: 2.242771, mean_q: 3.025222, mean epsilon: 0.487 [0.487, 0.487]\n",
      "   10738/5000000: episode: 193, duration: 3.545s, episode steps: 40, steps per second: 11, episode reward: 1.000, mean reward: 0.025 [0.000, 1.000], mean action: 2.200 [0.000, 3.000], mean observation: 39.937 [0.000, 142.000], loss: 0.197459, mean_absolute_error: 2.261574, mean_q: 3.048598, mean epsilon: 0.484 [0.484, 0.484]\n",
      "   10784/5000000: episode: 194, duration: 4.035s, episode steps: 46, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.848 [0.000, 3.000], mean observation: 39.817 [0.000, 142.000], loss: 0.194841, mean_absolute_error: 2.257709, mean_q: 3.045329, mean epsilon: 0.482 [0.482, 0.482]\n",
      "   10830/5000000: episode: 195, duration: 3.969s, episode steps: 46, steps per second: 12, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.696 [0.000, 3.000], mean observation: 39.709 [0.000, 142.000], loss: 0.193920, mean_absolute_error: 2.285155, mean_q: 3.094117, mean epsilon: 0.479 [0.479, 0.479]\n",
      "   10881/5000000: episode: 196, duration: 4.396s, episode steps: 51, steps per second: 12, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 1.922 [0.000, 3.000], mean observation: 39.830 [0.000, 142.000], loss: 0.190516, mean_absolute_error: 2.271417, mean_q: 3.059612, mean epsilon: 0.476 [0.476, 0.476]\n",
      "2018-07-07 23:21:36,242:atari_game:INFO: Episode 195/1000 finished. Mean reward over last 15 episodes: 1.69\n",
      "   10945/5000000: episode: 197, duration: 5.518s, episode steps: 64, steps per second: 12, episode reward: 3.000, mean reward: 0.047 [0.000, 1.000], mean action: 2.062 [0.000, 3.000], mean observation: 39.790 [0.000, 142.000], loss: 0.202159, mean_absolute_error: 2.272240, mean_q: 3.048849, mean epsilon: 0.474 [0.474, 0.474]\n",
      "   11023/5000000: episode: 198, duration: 6.859s, episode steps: 78, steps per second: 11, episode reward: 7.000, mean reward: 0.090 [0.000, 4.000], mean action: 2.372 [0.000, 3.000], mean observation: 39.621 [0.000, 142.000], loss: 0.205499, mean_absolute_error: 2.265731, mean_q: 3.014869, mean epsilon: 0.471 [0.471, 0.471]\n",
      "2018-07-07 23:21:49,199:atari_game:INFO: Target updated, step: 10999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   11093/5000000: episode: 199, duration: 6.199s, episode steps: 70, steps per second: 11, episode reward: 4.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.429 [0.000, 3.000], mean observation: 39.620 [0.000, 142.000], loss: 0.192632, mean_absolute_error: 2.217904, mean_q: 2.982197, mean epsilon: 0.468 [0.468, 0.468]\n",
      "   11130/5000000: episode: 200, duration: 3.219s, episode steps: 37, steps per second: 11, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 1.703 [0.000, 3.000], mean observation: 39.930 [0.000, 142.000], loss: 0.192207, mean_absolute_error: 2.152085, mean_q: 2.920371, mean epsilon: 0.465 [0.465, 0.465]\n",
      "   11187/5000000: episode: 201, duration: 5.097s, episode steps: 57, steps per second: 11, episode reward: 2.000, mean reward: 0.035 [0.000, 1.000], mean action: 1.895 [0.000, 3.000], mean observation: 39.853 [0.000, 142.000], loss: 0.198949, mean_absolute_error: 2.163181, mean_q: 2.932432, mean epsilon: 0.463 [0.463, 0.463]\n",
      "   11238/5000000: episode: 202, duration: 4.618s, episode steps: 51, steps per second: 11, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 1.922 [0.000, 3.000], mean observation: 39.820 [0.000, 142.000], loss: 0.201207, mean_absolute_error: 2.176768, mean_q: 2.936698, mean epsilon: 0.460 [0.460, 0.460]\n",
      "   11293/5000000: episode: 203, duration: 4.801s, episode steps: 55, steps per second: 11, episode reward: 3.000, mean reward: 0.055 [0.000, 1.000], mean action: 2.091 [0.000, 3.000], mean observation: 39.705 [0.000, 142.000], loss: 0.200080, mean_absolute_error: 2.169664, mean_q: 2.956296, mean epsilon: 0.457 [0.457, 0.457]\n",
      "   11341/5000000: episode: 204, duration: 4.225s, episode steps: 48, steps per second: 11, episode reward: 2.000, mean reward: 0.042 [0.000, 1.000], mean action: 2.062 [0.000, 3.000], mean observation: 39.741 [0.000, 142.000], loss: 0.200836, mean_absolute_error: 2.169725, mean_q: 2.974371, mean epsilon: 0.455 [0.455, 0.455]\n",
      "   11408/5000000: episode: 205, duration: 5.879s, episode steps: 67, steps per second: 11, episode reward: 7.000, mean reward: 0.104 [0.000, 4.000], mean action: 1.746 [0.000, 3.000], mean observation: 39.687 [0.000, 142.000], loss: 0.198681, mean_absolute_error: 2.165456, mean_q: 2.936400, mean epsilon: 0.452 [0.452, 0.452]\n",
      "   11435/5000000: episode: 206, duration: 2.381s, episode steps: 27, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.519 [0.000, 3.000], mean observation: 40.024 [0.000, 142.000], loss: 0.193804, mean_absolute_error: 2.160228, mean_q: 2.908404, mean epsilon: 0.449 [0.449, 0.449]\n",
      "   11471/5000000: episode: 207, duration: 3.303s, episode steps: 36, steps per second: 11, episode reward: 1.000, mean reward: 0.028 [0.000, 1.000], mean action: 2.139 [0.000, 3.000], mean observation: 39.902 [0.000, 142.000], loss: 0.209604, mean_absolute_error: 2.183130, mean_q: 2.959410, mean epsilon: 0.447 [0.447, 0.447]\n",
      "   11508/5000000: episode: 208, duration: 3.342s, episode steps: 37, steps per second: 11, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 1.432 [0.000, 3.000], mean observation: 39.738 [0.000, 142.000], loss: 0.193621, mean_absolute_error: 2.192324, mean_q: 2.965613, mean epsilon: 0.444 [0.444, 0.444]\n",
      "   11557/5000000: episode: 209, duration: 4.349s, episode steps: 49, steps per second: 11, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 2.061 [0.000, 3.000], mean observation: 39.809 [0.000, 142.000], loss: 0.193344, mean_absolute_error: 2.214062, mean_q: 2.998105, mean epsilon: 0.441 [0.441, 0.441]\n",
      "   11601/5000000: episode: 210, duration: 3.832s, episode steps: 44, steps per second: 11, episode reward: 2.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.955 [0.000, 3.000], mean observation: 39.743 [0.000, 142.000], loss: 0.206597, mean_absolute_error: 2.203076, mean_q: 2.961904, mean epsilon: 0.438 [0.438, 0.438]\n",
      "   11695/5000000: episode: 211, duration: 8.213s, episode steps: 94, steps per second: 11, episode reward: 6.000, mean reward: 0.064 [0.000, 1.000], mean action: 1.989 [0.000, 3.000], mean observation: 39.490 [0.000, 142.000], loss: 0.186033, mean_absolute_error: 2.150563, mean_q: 2.921494, mean epsilon: 0.436 [0.436, 0.436]\n",
      "2018-07-07 23:22:49,581:atari_game:INFO: Episode 210/1000 finished. Mean reward over last 15 episodes: 2.81\n",
      "   11746/5000000: episode: 212, duration: 4.494s, episode steps: 51, steps per second: 11, episode reward: 3.000, mean reward: 0.059 [0.000, 1.000], mean action: 1.824 [0.000, 3.000], mean observation: 39.691 [0.000, 142.000], loss: 0.201370, mean_absolute_error: 2.154882, mean_q: 2.947762, mean epsilon: 0.433 [0.433, 0.433]\n",
      "   11796/5000000: episode: 213, duration: 4.585s, episode steps: 50, steps per second: 11, episode reward: 1.000, mean reward: 0.020 [0.000, 1.000], mean action: 2.260 [0.000, 3.000], mean observation: 39.927 [0.000, 142.000], loss: 0.189550, mean_absolute_error: 2.170192, mean_q: 2.971415, mean epsilon: 0.430 [0.430, 0.430]\n",
      "   11852/5000000: episode: 214, duration: 5.012s, episode steps: 56, steps per second: 11, episode reward: 3.000, mean reward: 0.054 [0.000, 1.000], mean action: 2.018 [0.000, 3.000], mean observation: 39.698 [0.000, 142.000], loss: 0.195089, mean_absolute_error: 2.185096, mean_q: 2.976028, mean epsilon: 0.428 [0.428, 0.428]\n",
      "   11935/5000000: episode: 215, duration: 7.251s, episode steps: 83, steps per second: 11, episode reward: 8.000, mean reward: 0.096 [0.000, 4.000], mean action: 2.181 [0.000, 3.000], mean observation: 39.562 [0.000, 142.000], loss: 0.195218, mean_absolute_error: 2.183720, mean_q: 2.960302, mean epsilon: 0.425 [0.425, 0.425]\n",
      "   12006/5000000: episode: 216, duration: 6.205s, episode steps: 71, steps per second: 11, episode reward: 3.000, mean reward: 0.042 [0.000, 1.000], mean action: 2.113 [0.000, 3.000], mean observation: 39.707 [0.000, 142.000], loss: 0.203564, mean_absolute_error: 2.188354, mean_q: 2.976579, mean epsilon: 0.422 [0.422, 0.422]\n",
      "2018-07-07 23:23:20,469:atari_game:INFO: Target updated, step: 11999\n",
      "   12038/5000000: episode: 217, duration: 2.865s, episode steps: 32, steps per second: 11, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 1.781 [0.000, 3.000], mean observation: 39.929 [0.000, 142.000], loss: 0.196657, mean_absolute_error: 2.157840, mean_q: 2.939337, mean epsilon: 0.419 [0.419, 0.419]\n",
      "   12085/5000000: episode: 218, duration: 4.069s, episode steps: 47, steps per second: 12, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 2.128 [0.000, 3.000], mean observation: 39.749 [0.000, 142.000], loss: 0.202921, mean_absolute_error: 2.277027, mean_q: 3.102064, mean epsilon: 0.417 [0.417, 0.417]\n",
      "   12151/5000000: episode: 219, duration: 5.888s, episode steps: 66, steps per second: 11, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.970 [0.000, 3.000], mean observation: 39.659 [0.000, 142.000], loss: 0.204931, mean_absolute_error: 2.230128, mean_q: 3.020931, mean epsilon: 0.414 [0.414, 0.414]\n",
      "   12189/5000000: episode: 220, duration: 3.348s, episode steps: 38, steps per second: 11, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.789 [0.000, 3.000], mean observation: 39.792 [0.000, 142.000], loss: 0.205857, mean_absolute_error: 2.261141, mean_q: 3.092260, mean epsilon: 0.411 [0.411, 0.411]\n",
      "   12264/5000000: episode: 221, duration: 6.487s, episode steps: 75, steps per second: 12, episode reward: 7.000, mean reward: 0.093 [0.000, 4.000], mean action: 2.027 [0.000, 3.000], mean observation: 39.641 [0.000, 142.000], loss: 0.205274, mean_absolute_error: 2.235415, mean_q: 3.027137, mean epsilon: 0.409 [0.409, 0.409]\n",
      "   12302/5000000: episode: 222, duration: 3.284s, episode steps: 38, steps per second: 12, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.316 [0.000, 3.000], mean observation: 39.915 [0.000, 142.000], loss: 0.199454, mean_absolute_error: 2.284361, mean_q: 3.067462, mean epsilon: 0.406 [0.406, 0.406]\n",
      "   12358/5000000: episode: 223, duration: 4.831s, episode steps: 56, steps per second: 12, episode reward: 3.000, mean reward: 0.054 [0.000, 1.000], mean action: 1.893 [0.000, 3.000], mean observation: 39.712 [0.000, 142.000], loss: 0.216005, mean_absolute_error: 2.277586, mean_q: 3.041203, mean epsilon: 0.403 [0.403, 0.403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   12405/5000000: episode: 224, duration: 4.073s, episode steps: 47, steps per second: 12, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 2.085 [0.000, 3.000], mean observation: 39.805 [0.000, 142.000], loss: 0.223257, mean_absolute_error: 2.224673, mean_q: 2.983988, mean epsilon: 0.401 [0.401, 0.401]\n",
      "   12467/5000000: episode: 225, duration: 5.542s, episode steps: 62, steps per second: 11, episode reward: 4.000, mean reward: 0.065 [0.000, 1.000], mean action: 1.258 [0.000, 3.000], mean observation: 39.632 [0.000, 142.000], loss: 0.211287, mean_absolute_error: 2.299289, mean_q: 3.077204, mean epsilon: 0.398 [0.398, 0.398]\n",
      "   12528/5000000: episode: 226, duration: 5.372s, episode steps: 61, steps per second: 11, episode reward: 2.000, mean reward: 0.033 [0.000, 1.000], mean action: 1.967 [0.000, 3.000], mean observation: 39.726 [0.000, 142.000], loss: 0.206630, mean_absolute_error: 2.265028, mean_q: 3.099527, mean epsilon: 0.395 [0.395, 0.395]\n",
      "2018-07-07 23:24:04,416:atari_game:INFO: Episode 225/1000 finished. Mean reward over last 15 episodes: 3.12\n",
      "   12596/5000000: episode: 227, duration: 5.965s, episode steps: 68, steps per second: 11, episode reward: 3.000, mean reward: 0.044 [0.000, 1.000], mean action: 2.132 [0.000, 3.000], mean observation: 39.745 [0.000, 142.000], loss: 0.211950, mean_absolute_error: 2.236068, mean_q: 2.990843, mean epsilon: 0.392 [0.392, 0.392]\n",
      "   12650/5000000: episode: 228, duration: 4.709s, episode steps: 54, steps per second: 11, episode reward: 2.000, mean reward: 0.037 [0.000, 1.000], mean action: 2.167 [0.000, 3.000], mean observation: 39.799 [0.000, 142.000], loss: 0.202073, mean_absolute_error: 2.259078, mean_q: 3.087057, mean epsilon: 0.390 [0.390, 0.390]\n",
      "   12685/5000000: episode: 229, duration: 3.090s, episode steps: 35, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.200 [0.000, 3.000], mean observation: 39.781 [0.000, 142.000], loss: 0.195224, mean_absolute_error: 2.257560, mean_q: 3.053641, mean epsilon: 0.387 [0.387, 0.387]\n",
      "   12755/5000000: episode: 230, duration: 6.154s, episode steps: 70, steps per second: 11, episode reward: 3.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.929 [0.000, 3.000], mean observation: 39.604 [0.000, 142.000], loss: 0.196514, mean_absolute_error: 2.268118, mean_q: 3.061932, mean epsilon: 0.384 [0.384, 0.384]\n",
      "   12832/5000000: episode: 231, duration: 7.007s, episode steps: 77, steps per second: 11, episode reward: 5.000, mean reward: 0.065 [0.000, 1.000], mean action: 1.662 [0.000, 3.000], mean observation: 39.584 [0.000, 142.000], loss: 0.201778, mean_absolute_error: 2.251847, mean_q: 3.068281, mean epsilon: 0.382 [0.382, 0.382]\n",
      "   12883/5000000: episode: 232, duration: 4.543s, episode steps: 51, steps per second: 11, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 2.196 [0.000, 3.000], mean observation: 39.706 [0.000, 142.000], loss: 0.199615, mean_absolute_error: 2.268053, mean_q: 3.073617, mean epsilon: 0.379 [0.379, 0.379]\n",
      "   12936/5000000: episode: 233, duration: 4.652s, episode steps: 53, steps per second: 11, episode reward: 1.000, mean reward: 0.019 [0.000, 1.000], mean action: 2.000 [0.000, 3.000], mean observation: 39.919 [0.000, 142.000], loss: 0.207790, mean_absolute_error: 2.268228, mean_q: 3.057667, mean epsilon: 0.376 [0.376, 0.376]\n",
      "   13006/5000000: episode: 234, duration: 6.156s, episode steps: 70, steps per second: 11, episode reward: 3.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 39.735 [0.000, 142.000], loss: 0.209286, mean_absolute_error: 2.271014, mean_q: 3.096885, mean epsilon: 0.374 [0.374, 0.374]\n",
      "2018-07-07 23:24:50,943:atari_game:INFO: Target updated, step: 12999\n",
      "   13058/5000000: episode: 235, duration: 4.550s, episode steps: 52, steps per second: 11, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 1.481 [0.000, 3.000], mean observation: 39.699 [0.000, 142.000], loss: 0.213671, mean_absolute_error: 2.293390, mean_q: 3.096185, mean epsilon: 0.371 [0.371, 0.371]\n",
      "   13083/5000000: episode: 236, duration: 2.202s, episode steps: 25, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.520 [1.000, 3.000], mean observation: 40.020 [0.000, 142.000], loss: 0.197655, mean_absolute_error: 2.366015, mean_q: 3.181127, mean epsilon: 0.368 [0.368, 0.368]\n",
      "   13115/5000000: episode: 237, duration: 2.784s, episode steps: 32, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.688 [0.000, 3.000], mean observation: 40.006 [0.000, 142.000], loss: 0.220337, mean_absolute_error: 2.328269, mean_q: 3.132277, mean epsilon: 0.366 [0.366, 0.366]\n",
      "   13152/5000000: episode: 238, duration: 3.205s, episode steps: 37, steps per second: 12, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 1.378 [0.000, 3.000], mean observation: 39.934 [0.000, 142.000], loss: 0.198180, mean_absolute_error: 2.319831, mean_q: 3.130318, mean epsilon: 0.363 [0.363, 0.363]\n",
      "   13214/5000000: episode: 239, duration: 5.375s, episode steps: 62, steps per second: 12, episode reward: 2.000, mean reward: 0.032 [0.000, 1.000], mean action: 1.952 [0.000, 3.000], mean observation: 39.741 [0.000, 142.000], loss: 0.212331, mean_absolute_error: 2.292647, mean_q: 3.083851, mean epsilon: 0.360 [0.360, 0.360]\n",
      "   13266/5000000: episode: 240, duration: 4.667s, episode steps: 52, steps per second: 11, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 1.731 [0.000, 3.000], mean observation: 39.732 [0.000, 142.000], loss: 0.198996, mean_absolute_error: 2.359717, mean_q: 3.171574, mean epsilon: 0.357 [0.357, 0.357]\n",
      "   13319/5000000: episode: 241, duration: 4.697s, episode steps: 53, steps per second: 11, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.887 [0.000, 3.000], mean observation: 39.710 [0.000, 142.000], loss: 0.219517, mean_absolute_error: 2.320443, mean_q: 3.077196, mean epsilon: 0.355 [0.355, 0.355]\n",
      "2018-07-07 23:25:15,708:atari_game:INFO: Episode 240/1000 finished. Mean reward over last 15 episodes: 2.00\n",
      "   13391/5000000: episode: 242, duration: 6.516s, episode steps: 72, steps per second: 11, episode reward: 2.000, mean reward: 0.028 [0.000, 1.000], mean action: 1.903 [0.000, 3.000], mean observation: 39.874 [0.000, 142.000], loss: 0.215007, mean_absolute_error: 2.327006, mean_q: 3.120688, mean epsilon: 0.352 [0.352, 0.352]\n",
      "   13456/5000000: episode: 243, duration: 5.689s, episode steps: 65, steps per second: 11, episode reward: 2.000, mean reward: 0.031 [0.000, 1.000], mean action: 2.477 [0.000, 3.000], mean observation: 39.773 [0.000, 142.000], loss: 0.207481, mean_absolute_error: 2.357950, mean_q: 3.185038, mean epsilon: 0.349 [0.349, 0.349]\n",
      "   13524/5000000: episode: 244, duration: 6.167s, episode steps: 68, steps per second: 11, episode reward: 2.000, mean reward: 0.029 [0.000, 1.000], mean action: 2.088 [0.000, 3.000], mean observation: 39.617 [0.000, 142.000], loss: 0.203443, mean_absolute_error: 2.360145, mean_q: 3.165424, mean epsilon: 0.347 [0.347, 0.347]\n",
      "   13566/5000000: episode: 245, duration: 3.779s, episode steps: 42, steps per second: 11, episode reward: 1.000, mean reward: 0.024 [0.000, 1.000], mean action: 1.929 [0.000, 3.000], mean observation: 39.949 [0.000, 142.000], loss: 0.207672, mean_absolute_error: 2.342766, mean_q: 3.146357, mean epsilon: 0.344 [0.344, 0.344]\n",
      "   13633/5000000: episode: 246, duration: 5.931s, episode steps: 67, steps per second: 11, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.985 [0.000, 3.000], mean observation: 39.741 [0.000, 142.000], loss: 0.211998, mean_absolute_error: 2.333135, mean_q: 3.147375, mean epsilon: 0.341 [0.341, 0.341]\n",
      "   13701/5000000: episode: 247, duration: 6.039s, episode steps: 68, steps per second: 11, episode reward: 3.000, mean reward: 0.044 [0.000, 1.000], mean action: 1.941 [0.000, 3.000], mean observation: 39.649 [0.000, 142.000], loss: 0.220748, mean_absolute_error: 2.309816, mean_q: 3.080005, mean epsilon: 0.339 [0.339, 0.339]\n",
      "   13749/5000000: episode: 248, duration: 4.184s, episode steps: 48, steps per second: 11, episode reward: 2.000, mean reward: 0.042 [0.000, 1.000], mean action: 2.125 [0.000, 3.000], mean observation: 39.751 [0.000, 142.000], loss: 0.212565, mean_absolute_error: 2.371277, mean_q: 3.155785, mean epsilon: 0.336 [0.336, 0.336]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   13789/5000000: episode: 249, duration: 3.524s, episode steps: 40, steps per second: 11, episode reward: 1.000, mean reward: 0.025 [0.000, 1.000], mean action: 1.575 [0.000, 3.000], mean observation: 39.944 [0.000, 142.000], loss: 0.211596, mean_absolute_error: 2.354322, mean_q: 3.149104, mean epsilon: 0.333 [0.333, 0.333]\n",
      "   13842/5000000: episode: 250, duration: 4.630s, episode steps: 53, steps per second: 11, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.208 [0.000, 3.000], mean observation: 39.630 [0.000, 142.000], loss: 0.213818, mean_absolute_error: 2.343799, mean_q: 3.152233, mean epsilon: 0.330 [0.330, 0.330]\n",
      "   13909/5000000: episode: 251, duration: 5.901s, episode steps: 67, steps per second: 11, episode reward: 2.000, mean reward: 0.030 [0.000, 1.000], mean action: 1.791 [0.000, 3.000], mean observation: 39.704 [0.000, 142.000], loss: 0.196399, mean_absolute_error: 2.335657, mean_q: 3.149511, mean epsilon: 0.328 [0.328, 0.328]\n",
      "   13988/5000000: episode: 252, duration: 6.929s, episode steps: 79, steps per second: 11, episode reward: 2.000, mean reward: 0.025 [0.000, 1.000], mean action: 1.937 [0.000, 3.000], mean observation: 39.798 [0.000, 142.000], loss: 0.211925, mean_absolute_error: 2.343329, mean_q: 3.140434, mean epsilon: 0.325 [0.325, 0.325]\n",
      "   14039/5000000: episode: 253, duration: 4.494s, episode steps: 51, steps per second: 11, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 1.627 [0.000, 3.000], mean observation: 39.820 [0.000, 142.000], loss: 0.197450, mean_absolute_error: 2.350899, mean_q: 3.177445, mean epsilon: 0.322 [0.322, 0.322]\n",
      "2018-07-07 23:26:22,035:atari_game:INFO: Target updated, step: 13999\n",
      "   14105/5000000: episode: 254, duration: 6.013s, episode steps: 66, steps per second: 11, episode reward: 4.000, mean reward: 0.061 [0.000, 1.000], mean action: 1.212 [0.000, 3.000], mean observation: 39.614 [0.000, 142.000], loss: 0.198930, mean_absolute_error: 2.325022, mean_q: 3.149226, mean epsilon: 0.320 [0.320, 0.320]\n",
      "   14142/5000000: episode: 255, duration: 3.327s, episode steps: 37, steps per second: 11, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 1.649 [0.000, 3.000], mean observation: 39.729 [0.000, 142.000], loss: 0.201570, mean_absolute_error: 2.296995, mean_q: 3.129698, mean epsilon: 0.317 [0.317, 0.317]\n",
      "   14194/5000000: episode: 256, duration: 4.600s, episode steps: 52, steps per second: 11, episode reward: 3.000, mean reward: 0.058 [0.000, 1.000], mean action: 1.462 [0.000, 3.000], mean observation: 39.708 [0.000, 142.000], loss: 0.206769, mean_absolute_error: 2.294205, mean_q: 3.077440, mean epsilon: 0.314 [0.314, 0.314]\n",
      "2018-07-07 23:26:34,930:atari_game:INFO: Episode 255/1000 finished. Mean reward over last 15 episodes: 2.25\n",
      "   14253/5000000: episode: 257, duration: 5.278s, episode steps: 59, steps per second: 11, episode reward: 3.000, mean reward: 0.051 [0.000, 1.000], mean action: 2.441 [0.000, 3.000], mean observation: 39.743 [0.000, 142.000], loss: 0.204807, mean_absolute_error: 2.293307, mean_q: 3.076245, mean epsilon: 0.311 [0.311, 0.311]\n",
      "   14288/5000000: episode: 258, duration: 3.105s, episode steps: 35, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.629 [1.000, 3.000], mean observation: 39.804 [0.000, 142.000], loss: 0.209531, mean_absolute_error: 2.270720, mean_q: 3.059932, mean epsilon: 0.309 [0.309, 0.309]\n",
      "   14327/5000000: episode: 259, duration: 3.498s, episode steps: 39, steps per second: 11, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.359 [0.000, 3.000], mean observation: 39.865 [0.000, 142.000], loss: 0.199965, mean_absolute_error: 2.354937, mean_q: 3.191617, mean epsilon: 0.306 [0.306, 0.306]\n",
      "   14371/5000000: episode: 260, duration: 3.888s, episode steps: 44, steps per second: 11, episode reward: 2.000, mean reward: 0.045 [0.000, 1.000], mean action: 2.341 [0.000, 3.000], mean observation: 39.860 [0.000, 142.000], loss: 0.218450, mean_absolute_error: 2.323154, mean_q: 3.067769, mean epsilon: 0.303 [0.303, 0.303]\n",
      "   14422/5000000: episode: 261, duration: 4.559s, episode steps: 51, steps per second: 11, episode reward: 3.000, mean reward: 0.059 [0.000, 1.000], mean action: 1.353 [0.000, 3.000], mean observation: 39.735 [0.000, 142.000], loss: 0.203241, mean_absolute_error: 2.318181, mean_q: 3.123920, mean epsilon: 0.301 [0.301, 0.301]\n",
      "   14457/5000000: episode: 262, duration: 3.093s, episode steps: 35, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.743 [0.000, 3.000], mean observation: 39.900 [0.000, 142.000], loss: 0.196931, mean_absolute_error: 2.299203, mean_q: 3.104199, mean epsilon: 0.298 [0.298, 0.298]\n",
      "   14490/5000000: episode: 263, duration: 2.982s, episode steps: 33, steps per second: 11, episode reward: 1.000, mean reward: 0.030 [0.000, 1.000], mean action: 1.515 [0.000, 3.000], mean observation: 39.940 [0.000, 142.000], loss: 0.199715, mean_absolute_error: 2.290080, mean_q: 3.065600, mean epsilon: 0.295 [0.295, 0.295]\n",
      "   14557/5000000: episode: 264, duration: 5.945s, episode steps: 67, steps per second: 11, episode reward: 4.000, mean reward: 0.060 [0.000, 1.000], mean action: 1.448 [0.000, 3.000], mean observation: 39.620 [0.000, 142.000], loss: 0.204817, mean_absolute_error: 2.263581, mean_q: 3.060561, mean epsilon: 0.293 [0.293, 0.293]\n",
      "   14634/5000000: episode: 265, duration: 6.829s, episode steps: 77, steps per second: 11, episode reward: 2.000, mean reward: 0.026 [0.000, 1.000], mean action: 2.286 [0.000, 3.000], mean observation: 39.685 [0.000, 142.000], loss: 0.200075, mean_absolute_error: 2.296398, mean_q: 3.068064, mean epsilon: 0.290 [0.290, 0.290]\n",
      "   14701/5000000: episode: 266, duration: 5.966s, episode steps: 67, steps per second: 11, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 2.090 [0.000, 3.000], mean observation: 39.803 [0.000, 142.000], loss: 0.209080, mean_absolute_error: 2.293310, mean_q: 3.085314, mean epsilon: 0.287 [0.287, 0.287]\n",
      "   14789/5000000: episode: 267, duration: 7.699s, episode steps: 88, steps per second: 11, episode reward: 3.000, mean reward: 0.034 [0.000, 1.000], mean action: 1.898 [0.000, 3.000], mean observation: 39.637 [0.000, 142.000], loss: 0.202527, mean_absolute_error: 2.287539, mean_q: 3.079848, mean epsilon: 0.284 [0.284, 0.284]\n",
      "   14864/5000000: episode: 268, duration: 6.818s, episode steps: 75, steps per second: 11, episode reward: 3.000, mean reward: 0.040 [0.000, 1.000], mean action: 2.360 [0.000, 3.000], mean observation: 39.658 [0.000, 142.000], loss: 0.206273, mean_absolute_error: 2.285072, mean_q: 3.035397, mean epsilon: 0.282 [0.282, 0.282]\n",
      "   14899/5000000: episode: 269, duration: 3.163s, episode steps: 35, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.771 [0.000, 3.000], mean observation: 39.935 [0.000, 142.000], loss: 0.198493, mean_absolute_error: 2.302923, mean_q: 3.122884, mean epsilon: 0.279 [0.279, 0.279]\n",
      "   14985/5000000: episode: 270, duration: 7.512s, episode steps: 86, steps per second: 11, episode reward: 7.000, mean reward: 0.081 [0.000, 4.000], mean action: 2.256 [0.000, 3.000], mean observation: 39.582 [0.000, 142.000], loss: 0.194363, mean_absolute_error: 2.268854, mean_q: 3.065088, mean epsilon: 0.276 [0.276, 0.276]\n",
      "   15032/5000000: episode: 271, duration: 4.135s, episode steps: 47, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.319 [0.000, 3.000], mean observation: 39.695 [0.000, 142.000], loss: 0.199526, mean_absolute_error: 2.300354, mean_q: 3.127512, mean epsilon: 0.274 [0.274, 0.274]\n",
      "2018-07-07 23:27:50,981:atari_game:INFO: Episode 270/1000 finished. Mean reward over last 15 episodes: 2.50\n",
      "2018-07-07 23:27:53,695:atari_game:INFO: Target updated, step: 14999\n",
      "   15082/5000000: episode: 272, duration: 4.369s, episode steps: 50, steps per second: 11, episode reward: 2.000, mean reward: 0.040 [0.000, 1.000], mean action: 2.120 [0.000, 3.000], mean observation: 39.779 [0.000, 142.000], loss: 0.205375, mean_absolute_error: 2.192895, mean_q: 2.955186, mean epsilon: 0.271 [0.271, 0.271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   15129/5000000: episode: 273, duration: 4.196s, episode steps: 47, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 2.340 [0.000, 3.000], mean observation: 39.708 [0.000, 142.000], loss: 0.208979, mean_absolute_error: 2.074663, mean_q: 2.811205, mean epsilon: 0.268 [0.268, 0.268]\n",
      "   15169/5000000: episode: 274, duration: 3.682s, episode steps: 40, steps per second: 11, episode reward: 1.000, mean reward: 0.025 [0.000, 1.000], mean action: 1.550 [0.000, 3.000], mean observation: 39.874 [0.000, 142.000], loss: 0.194181, mean_absolute_error: 2.071522, mean_q: 2.795170, mean epsilon: 0.266 [0.266, 0.266]\n",
      "   15204/5000000: episode: 275, duration: 3.142s, episode steps: 35, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.457 [0.000, 3.000], mean observation: 39.852 [0.000, 142.000], loss: 0.188793, mean_absolute_error: 2.097161, mean_q: 2.823127, mean epsilon: 0.263 [0.263, 0.263]\n",
      "   15291/5000000: episode: 276, duration: 7.617s, episode steps: 87, steps per second: 11, episode reward: 5.000, mean reward: 0.057 [0.000, 1.000], mean action: 2.276 [0.000, 3.000], mean observation: 39.569 [0.000, 142.000], loss: 0.200258, mean_absolute_error: 2.071207, mean_q: 2.790268, mean epsilon: 0.260 [0.260, 0.260]\n",
      "   15351/5000000: episode: 277, duration: 5.270s, episode steps: 60, steps per second: 11, episode reward: 3.000, mean reward: 0.050 [0.000, 1.000], mean action: 1.983 [0.000, 3.000], mean observation: 39.749 [0.000, 142.000], loss: 0.199642, mean_absolute_error: 2.068239, mean_q: 2.824578, mean epsilon: 0.257 [0.257, 0.257]\n",
      "   15389/5000000: episode: 278, duration: 3.336s, episode steps: 38, steps per second: 11, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 1.684 [0.000, 3.000], mean observation: 39.729 [0.000, 142.000], loss: 0.186779, mean_absolute_error: 2.063768, mean_q: 2.812149, mean epsilon: 0.255 [0.255, 0.255]\n",
      "   15444/5000000: episode: 279, duration: 4.842s, episode steps: 55, steps per second: 11, episode reward: 3.000, mean reward: 0.055 [0.000, 1.000], mean action: 2.109 [0.000, 3.000], mean observation: 39.736 [0.000, 142.000], loss: 0.201055, mean_absolute_error: 2.107481, mean_q: 2.878109, mean epsilon: 0.252 [0.252, 0.252]\n",
      "   15469/5000000: episode: 280, duration: 2.168s, episode steps: 25, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.160 [0.000, 3.000], mean observation: 40.038 [0.000, 142.000], loss: 0.185357, mean_absolute_error: 2.093927, mean_q: 2.869234, mean epsilon: 0.249 [0.249, 0.249]\n",
      "   15501/5000000: episode: 281, duration: 2.745s, episode steps: 32, steps per second: 12, episode reward: 1.000, mean reward: 0.031 [0.000, 1.000], mean action: 0.906 [0.000, 1.000], mean observation: 39.692 [0.000, 142.000], loss: 0.197861, mean_absolute_error: 2.075447, mean_q: 2.832267, mean epsilon: 0.247 [0.247, 0.247]\n",
      "   15585/5000000: episode: 282, duration: 7.259s, episode steps: 84, steps per second: 12, episode reward: 7.000, mean reward: 0.083 [0.000, 4.000], mean action: 2.321 [0.000, 3.000], mean observation: 39.613 [0.000, 142.000], loss: 0.191347, mean_absolute_error: 2.060833, mean_q: 2.783710, mean epsilon: 0.244 [0.244, 0.244]\n",
      "   15619/5000000: episode: 283, duration: 2.931s, episode steps: 34, steps per second: 12, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.118 [0.000, 3.000], mean observation: 39.683 [0.000, 142.000], loss: 0.190410, mean_absolute_error: 2.079943, mean_q: 2.840888, mean epsilon: 0.241 [0.241, 0.241]\n",
      "   15664/5000000: episode: 284, duration: 3.939s, episode steps: 45, steps per second: 11, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 2.200 [0.000, 3.000], mean observation: 39.961 [0.000, 142.000], loss: 0.196618, mean_absolute_error: 2.102820, mean_q: 2.827402, mean epsilon: 0.239 [0.239, 0.239]\n",
      "   15698/5000000: episode: 285, duration: 2.926s, episode steps: 34, steps per second: 12, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.412 [0.000, 3.000], mean observation: 39.700 [0.000, 142.000], loss: 0.190912, mean_absolute_error: 2.091841, mean_q: 2.854951, mean epsilon: 0.236 [0.236, 0.236]\n",
      "   15774/5000000: episode: 286, duration: 6.714s, episode steps: 76, steps per second: 11, episode reward: 4.000, mean reward: 0.053 [0.000, 1.000], mean action: 1.776 [0.000, 3.000], mean observation: 39.633 [0.000, 142.000], loss: 0.201311, mean_absolute_error: 2.101426, mean_q: 2.875627, mean epsilon: 0.233 [0.233, 0.233]\n",
      "2018-07-07 23:28:57,624:atari_game:INFO: Episode 285/1000 finished. Mean reward over last 15 episodes: 2.19\n",
      "   15847/5000000: episode: 287, duration: 6.471s, episode steps: 73, steps per second: 11, episode reward: 4.000, mean reward: 0.055 [0.000, 1.000], mean action: 2.247 [0.000, 3.000], mean observation: 39.655 [0.000, 142.000], loss: 0.192676, mean_absolute_error: 2.075702, mean_q: 2.827050, mean epsilon: 0.231 [0.231, 0.231]\n",
      "   15921/5000000: episode: 288, duration: 6.515s, episode steps: 74, steps per second: 11, episode reward: 7.000, mean reward: 0.095 [0.000, 4.000], mean action: 2.378 [0.000, 3.000], mean observation: 39.614 [0.000, 142.000], loss: 0.201636, mean_absolute_error: 2.088132, mean_q: 2.811804, mean epsilon: 0.228 [0.228, 0.228]\n",
      "   15964/5000000: episode: 289, duration: 3.736s, episode steps: 43, steps per second: 12, episode reward: 2.000, mean reward: 0.047 [0.000, 1.000], mean action: 1.070 [0.000, 2.000], mean observation: 39.652 [0.000, 142.000], loss: 0.196680, mean_absolute_error: 2.060678, mean_q: 2.809942, mean epsilon: 0.225 [0.225, 0.225]\n",
      "   16047/5000000: episode: 290, duration: 7.258s, episode steps: 83, steps per second: 11, episode reward: 2.000, mean reward: 0.024 [0.000, 1.000], mean action: 2.386 [0.000, 3.000], mean observation: 39.856 [0.000, 142.000], loss: 0.209448, mean_absolute_error: 2.071847, mean_q: 2.847327, mean epsilon: 0.222 [0.222, 0.222]\n",
      "2018-07-07 23:29:24,311:atari_game:INFO: Target updated, step: 15999\n",
      "   16090/5000000: episode: 291, duration: 3.742s, episode steps: 43, steps per second: 11, episode reward: 1.000, mean reward: 0.023 [0.000, 1.000], mean action: 2.674 [0.000, 3.000], mean observation: 39.921 [0.000, 142.000], loss: 0.185054, mean_absolute_error: 2.046322, mean_q: 2.802195, mean epsilon: 0.220 [0.220, 0.220]\n",
      "   16136/5000000: episode: 292, duration: 4.126s, episode steps: 46, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 2.043 [1.000, 3.000], mean observation: 39.742 [0.000, 142.000], loss: 0.210995, mean_absolute_error: 2.091911, mean_q: 2.898525, mean epsilon: 0.217 [0.217, 0.217]\n",
      "   16162/5000000: episode: 293, duration: 2.431s, episode steps: 26, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.077 [0.000, 3.000], mean observation: 39.985 [0.000, 142.000], loss: 0.186539, mean_absolute_error: 2.109696, mean_q: 2.880291, mean epsilon: 0.214 [0.214, 0.214]\n",
      "   16272/5000000: episode: 294, duration: 10.156s, episode steps: 110, steps per second: 11, episode reward: 3.000, mean reward: 0.027 [0.000, 1.000], mean action: 2.482 [0.000, 3.000], mean observation: 39.669 [0.000, 142.000], loss: 0.193716, mean_absolute_error: 2.063833, mean_q: 2.846998, mean epsilon: 0.212 [0.212, 0.212]\n",
      "   16317/5000000: episode: 295, duration: 3.961s, episode steps: 45, steps per second: 11, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 1.756 [0.000, 3.000], mean observation: 39.826 [0.000, 142.000], loss: 0.200051, mean_absolute_error: 2.073083, mean_q: 2.842063, mean epsilon: 0.209 [0.209, 0.209]\n",
      "   16370/5000000: episode: 296, duration: 4.589s, episode steps: 53, steps per second: 12, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 2.377 [0.000, 3.000], mean observation: 39.699 [0.000, 142.000], loss: 0.203185, mean_absolute_error: 2.059045, mean_q: 2.840596, mean epsilon: 0.206 [0.206, 0.206]\n",
      "   16429/5000000: episode: 297, duration: 5.100s, episode steps: 59, steps per second: 12, episode reward: 2.000, mean reward: 0.034 [0.000, 1.000], mean action: 2.000 [0.000, 3.000], mean observation: 39.855 [0.000, 142.000], loss: 0.210759, mean_absolute_error: 2.071065, mean_q: 2.845752, mean epsilon: 0.204 [0.204, 0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   16481/5000000: episode: 298, duration: 4.641s, episode steps: 52, steps per second: 11, episode reward: 2.000, mean reward: 0.038 [0.000, 1.000], mean action: 2.173 [0.000, 3.000], mean observation: 39.855 [0.000, 142.000], loss: 0.198803, mean_absolute_error: 2.063377, mean_q: 2.831783, mean epsilon: 0.201 [0.201, 0.201]\n",
      "   16518/5000000: episode: 299, duration: 3.451s, episode steps: 37, steps per second: 11, episode reward: 1.000, mean reward: 0.027 [0.000, 1.000], mean action: 1.135 [0.000, 3.000], mean observation: 39.968 [0.000, 142.000], loss: 0.202848, mean_absolute_error: 2.097964, mean_q: 2.826723, mean epsilon: 0.198 [0.198, 0.198]\n",
      "   16563/5000000: episode: 300, duration: 4.204s, episode steps: 45, steps per second: 11, episode reward: 1.000, mean reward: 0.022 [0.000, 1.000], mean action: 2.333 [0.000, 3.000], mean observation: 39.962 [0.000, 142.000], loss: 0.200685, mean_absolute_error: 2.075481, mean_q: 2.858177, mean epsilon: 0.195 [0.195, 0.195]\n",
      "   16607/5000000: episode: 301, duration: 3.979s, episode steps: 44, steps per second: 11, episode reward: 2.000, mean reward: 0.045 [0.000, 1.000], mean action: 1.886 [0.000, 3.000], mean observation: 39.789 [0.000, 142.000], loss: 0.213229, mean_absolute_error: 2.102066, mean_q: 2.926455, mean epsilon: 0.193 [0.193, 0.193]\n",
      "2018-07-07 23:30:13,555:atari_game:INFO: Episode 300/1000 finished. Mean reward over last 15 episodes: 2.31\n",
      "   16677/5000000: episode: 302, duration: 6.166s, episode steps: 70, steps per second: 11, episode reward: 3.000, mean reward: 0.043 [0.000, 1.000], mean action: 1.914 [0.000, 3.000], mean observation: 39.819 [0.000, 142.000], loss: 0.197689, mean_absolute_error: 2.063070, mean_q: 2.801992, mean epsilon: 0.190 [0.190, 0.190]\n",
      "   16748/5000000: episode: 303, duration: 6.164s, episode steps: 71, steps per second: 12, episode reward: 3.000, mean reward: 0.042 [0.000, 1.000], mean action: 2.394 [0.000, 3.000], mean observation: 39.699 [0.000, 142.000], loss: 0.196476, mean_absolute_error: 2.094354, mean_q: 2.936844, mean epsilon: 0.187 [0.187, 0.187]\n",
      "   16890/5000000: episode: 304, duration: 12.286s, episode steps: 142, steps per second: 12, episode reward: 7.000, mean reward: 0.049 [0.000, 4.000], mean action: 2.401 [0.000, 3.000], mean observation: 39.505 [0.000, 142.000], loss: 0.202488, mean_absolute_error: 2.078480, mean_q: 2.860677, mean epsilon: 0.185 [0.185, 0.185]\n",
      "   16941/5000000: episode: 305, duration: 4.590s, episode steps: 51, steps per second: 11, episode reward: 2.000, mean reward: 0.039 [0.000, 1.000], mean action: 2.176 [0.000, 3.000], mean observation: 39.829 [0.000, 142.000], loss: 0.203844, mean_absolute_error: 2.087033, mean_q: 2.877340, mean epsilon: 0.182 [0.182, 0.182]\n",
      "   17021/5000000: episode: 306, duration: 7.020s, episode steps: 80, steps per second: 11, episode reward: 5.000, mean reward: 0.062 [0.000, 1.000], mean action: 2.400 [0.000, 3.000], mean observation: 39.493 [0.000, 142.000], loss: 0.199510, mean_absolute_error: 2.071596, mean_q: 2.851610, mean epsilon: 0.179 [0.179, 0.179]\n",
      "   17072/5000000: episode: 307, duration: 4.507s, episode steps: 51, steps per second: 11, episode reward: 3.000, mean reward: 0.059 [0.000, 1.000], mean action: 1.804 [0.000, 3.000], mean observation: 39.697 [0.000, 142.000], loss: 0.199637, mean_absolute_error: 2.092079, mean_q: 2.844796, mean epsilon: 0.177 [0.176, 0.176]\n",
      "2018-07-07 23:30:55,445:atari_game:INFO: Target updated, step: 16999\n",
      "   17139/5000000: episode: 308, duration: 5.877s, episode steps: 67, steps per second: 11, episode reward: 2.000, mean reward: 0.030 [0.000, 1.000], mean action: 2.224 [1.000, 3.000], mean observation: 39.697 [0.000, 142.000], loss: 0.186244, mean_absolute_error: 2.114719, mean_q: 2.833221, mean epsilon: 0.174 [0.174, 0.174]\n",
      "   17225/5000000: episode: 309, duration: 7.561s, episode steps: 86, steps per second: 11, episode reward: 3.000, mean reward: 0.035 [0.000, 1.000], mean action: 1.977 [0.000, 3.000], mean observation: 39.703 [0.000, 142.000], loss: 0.195702, mean_absolute_error: 2.167468, mean_q: 2.870711, mean epsilon: 0.171 [0.171, 0.171]\n",
      "   17292/5000000: episode: 310, duration: 5.809s, episode steps: 67, steps per second: 12, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 2.284 [0.000, 3.000], mean observation: 39.694 [0.000, 142.000], loss: 0.198101, mean_absolute_error: 2.153225, mean_q: 2.830545, mean epsilon: 0.168 [0.168, 0.168]\n",
      "   17338/5000000: episode: 311, duration: 4.198s, episode steps: 46, steps per second: 11, episode reward: 2.000, mean reward: 0.043 [0.000, 1.000], mean action: 2.152 [0.000, 3.000], mean observation: 39.788 [0.000, 142.000], loss: 0.208684, mean_absolute_error: 2.157736, mean_q: 2.862266, mean epsilon: 0.166 [0.166, 0.166]\n",
      "   17426/5000000: episode: 312, duration: 7.901s, episode steps: 88, steps per second: 11, episode reward: 7.000, mean reward: 0.080 [0.000, 4.000], mean action: 2.489 [0.000, 3.000], mean observation: 39.558 [0.000, 142.000], loss: 0.213154, mean_absolute_error: 2.170910, mean_q: 2.861550, mean epsilon: 0.163 [0.163, 0.163]\n",
      "   17513/5000000: episode: 313, duration: 7.756s, episode steps: 87, steps per second: 11, episode reward: 3.000, mean reward: 0.034 [0.000, 1.000], mean action: 2.276 [0.000, 3.000], mean observation: 39.703 [0.000, 142.000], loss: 0.208415, mean_absolute_error: 2.160372, mean_q: 2.874448, mean epsilon: 0.160 [0.160, 0.160]\n",
      "   17573/5000000: episode: 314, duration: 5.282s, episode steps: 60, steps per second: 11, episode reward: 4.000, mean reward: 0.067 [0.000, 1.000], mean action: 1.833 [0.000, 3.000], mean observation: 39.632 [0.000, 142.000], loss: 0.195833, mean_absolute_error: 2.130730, mean_q: 2.841615, mean epsilon: 0.158 [0.158, 0.158]\n",
      "   17655/5000000: episode: 315, duration: 7.240s, episode steps: 82, steps per second: 11, episode reward: 3.000, mean reward: 0.037 [0.000, 1.000], mean action: 2.646 [0.000, 3.000], mean observation: 39.830 [0.000, 142.000], loss: 0.203966, mean_absolute_error: 2.152990, mean_q: 2.853473, mean epsilon: 0.155 [0.155, 0.155]\n",
      "   17696/5000000: episode: 316, duration: 3.765s, episode steps: 41, steps per second: 11, episode reward: 2.000, mean reward: 0.049 [0.000, 1.000], mean action: 2.098 [0.000, 3.000], mean observation: 39.764 [0.000, 142.000], loss: 0.197502, mean_absolute_error: 2.104453, mean_q: 2.769425, mean epsilon: 0.152 [0.152, 0.152]\n",
      "2018-07-07 23:31:51,218:atari_game:INFO: Episode 315/1000 finished. Mean reward over last 15 episodes: 3.38\n",
      "   17731/5000000: episode: 317, duration: 3.094s, episode steps: 35, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.943 [1.000, 3.000], mean observation: 39.933 [0.000, 142.000], loss: 0.200194, mean_absolute_error: 2.172891, mean_q: 2.911655, mean epsilon: 0.149 [0.149, 0.149]\n",
      "   17810/5000000: episode: 318, duration: 7.054s, episode steps: 79, steps per second: 11, episode reward: 3.000, mean reward: 0.038 [0.000, 1.000], mean action: 2.608 [0.000, 3.000], mean observation: 39.758 [0.000, 142.000], loss: 0.198117, mean_absolute_error: 2.129221, mean_q: 2.824046, mean epsilon: 0.147 [0.147, 0.147]\n",
      "   17868/5000000: episode: 319, duration: 5.071s, episode steps: 58, steps per second: 11, episode reward: 3.000, mean reward: 0.052 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 39.691 [0.000, 142.000], loss: 0.201216, mean_absolute_error: 2.141605, mean_q: 2.861985, mean epsilon: 0.144 [0.144, 0.144]\n",
      "   17975/5000000: episode: 320, duration: 9.646s, episode steps: 107, steps per second: 11, episode reward: 3.000, mean reward: 0.028 [0.000, 1.000], mean action: 2.262 [0.000, 3.000], mean observation: 39.599 [0.000, 142.000], loss: 0.198753, mean_absolute_error: 2.154741, mean_q: 2.870438, mean epsilon: 0.141 [0.141, 0.141]\n",
      "   18015/5000000: episode: 321, duration: 3.578s, episode steps: 40, steps per second: 11, episode reward: 2.000, mean reward: 0.050 [0.000, 1.000], mean action: 2.125 [0.000, 3.000], mean observation: 39.765 [0.000, 142.000], loss: 0.194663, mean_absolute_error: 2.156943, mean_q: 2.879407, mean epsilon: 0.139 [0.139, 0.139]\n",
      "2018-07-07 23:32:26,107:atari_game:INFO: Target updated, step: 17999\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18116/5000000: episode: 322, duration: 8.797s, episode steps: 101, steps per second: 11, episode reward: 7.000, mean reward: 0.069 [0.000, 4.000], mean action: 2.426 [0.000, 3.000], mean observation: 39.640 [0.000, 142.000], loss: 0.194989, mean_absolute_error: 2.123819, mean_q: 2.816765, mean epsilon: 0.136 [0.136, 0.136]\n",
      "   18169/5000000: episode: 323, duration: 4.587s, episode steps: 53, steps per second: 12, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.811 [0.000, 3.000], mean observation: 39.711 [0.000, 142.000], loss: 0.196403, mean_absolute_error: 2.012117, mean_q: 2.706038, mean epsilon: 0.133 [0.133, 0.133]\n",
      "   18234/5000000: episode: 324, duration: 5.864s, episode steps: 65, steps per second: 11, episode reward: 2.000, mean reward: 0.031 [0.000, 1.000], mean action: 2.585 [0.000, 3.000], mean observation: 39.861 [0.000, 142.000], loss: 0.195889, mean_absolute_error: 2.041006, mean_q: 2.739578, mean epsilon: 0.131 [0.131, 0.131]\n",
      "   18340/5000000: episode: 325, duration: 9.858s, episode steps: 106, steps per second: 11, episode reward: 3.000, mean reward: 0.028 [0.000, 1.000], mean action: 2.660 [0.000, 3.000], mean observation: 39.783 [0.000, 142.000], loss: 0.193265, mean_absolute_error: 2.017317, mean_q: 2.693600, mean epsilon: 0.128 [0.128, 0.128]\n",
      "   18368/5000000: episode: 326, duration: 2.467s, episode steps: 28, steps per second: 11, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.107 [0.000, 3.000], mean observation: 40.042 [0.000, 142.000], loss: 0.220488, mean_absolute_error: 2.073073, mean_q: 2.783816, mean epsilon: 0.125 [0.125, 0.125]\n",
      "   18393/5000000: episode: 327, duration: 2.173s, episode steps: 25, steps per second: 12, episode reward: 0.000, mean reward: 0.000 [0.000, 0.000], mean action: 1.280 [1.000, 3.000], mean observation: 39.967 [0.000, 142.000], loss: 0.195023, mean_absolute_error: 1.999604, mean_q: 2.687757, mean epsilon: 0.122 [0.122, 0.122]\n",
      "   18427/5000000: episode: 328, duration: 3.013s, episode steps: 34, steps per second: 11, episode reward: 1.000, mean reward: 0.029 [0.000, 1.000], mean action: 1.500 [0.000, 3.000], mean observation: 39.713 [0.000, 142.000], loss: 0.205187, mean_absolute_error: 2.036329, mean_q: 2.733631, mean epsilon: 0.120 [0.120, 0.120]\n",
      "   18502/5000000: episode: 329, duration: 6.622s, episode steps: 75, steps per second: 11, episode reward: 3.000, mean reward: 0.040 [0.000, 1.000], mean action: 2.707 [0.000, 3.000], mean observation: 39.790 [0.000, 142.000], loss: 0.193770, mean_absolute_error: 2.024350, mean_q: 2.701452, mean epsilon: 0.117 [0.117, 0.117]\n",
      "   18541/5000000: episode: 330, duration: 3.448s, episode steps: 39, steps per second: 11, episode reward: 1.000, mean reward: 0.026 [0.000, 1.000], mean action: 2.205 [0.000, 3.000], mean observation: 39.942 [0.000, 142.000], loss: 0.189607, mean_absolute_error: 2.026223, mean_q: 2.740407, mean epsilon: 0.114 [0.114, 0.114]\n",
      "   18606/5000000: episode: 331, duration: 5.721s, episode steps: 65, steps per second: 11, episode reward: 3.000, mean reward: 0.046 [0.000, 1.000], mean action: 2.431 [0.000, 3.000], mean observation: 39.716 [0.000, 142.000], loss: 0.187636, mean_absolute_error: 2.035822, mean_q: 2.746607, mean epsilon: 0.112 [0.112, 0.112]\n",
      "2018-07-07 23:33:13,876:atari_game:INFO: Episode 330/1000 finished. Mean reward over last 15 episodes: 2.31\n",
      "   18685/5000000: episode: 332, duration: 7.102s, episode steps: 79, steps per second: 11, episode reward: 2.000, mean reward: 0.025 [0.000, 1.000], mean action: 2.532 [0.000, 3.000], mean observation: 39.920 [0.000, 142.000], loss: 0.201594, mean_absolute_error: 2.020108, mean_q: 2.698484, mean epsilon: 0.109 [0.109, 0.109]\n",
      "   18734/5000000: episode: 333, duration: 4.357s, episode steps: 49, steps per second: 11, episode reward: 2.000, mean reward: 0.041 [0.000, 1.000], mean action: 2.000 [0.000, 3.000], mean observation: 39.736 [0.000, 142.000], loss: 0.196025, mean_absolute_error: 2.054165, mean_q: 2.761233, mean epsilon: 0.106 [0.106, 0.106]\n",
      "   18864/5000000: episode: 334, duration: 11.787s, episode steps: 130, steps per second: 11, episode reward: 3.000, mean reward: 0.023 [0.000, 1.000], mean action: 2.554 [0.000, 3.000], mean observation: 39.683 [0.000, 142.000], loss: 0.200498, mean_absolute_error: 2.033494, mean_q: 2.716403, mean epsilon: 0.104 [0.104, 0.104]\n",
      "   18938/5000000: episode: 335, duration: 7.204s, episode steps: 74, steps per second: 10, episode reward: 3.000, mean reward: 0.041 [0.000, 1.000], mean action: 2.622 [0.000, 3.000], mean observation: 39.705 [0.000, 142.000], loss: 0.190254, mean_absolute_error: 2.014236, mean_q: 2.709237, mean epsilon: 0.101 [0.101, 0.101]\n",
      "   18991/5000000: episode: 336, duration: 4.761s, episode steps: 53, steps per second: 11, episode reward: 3.000, mean reward: 0.057 [0.000, 1.000], mean action: 1.698 [0.000, 3.000], mean observation: 39.703 [0.000, 142.000], loss: 0.201800, mean_absolute_error: 2.041213, mean_q: 2.713182, mean epsilon: 0.098 [0.098, 0.098]\n",
      "   19071/5000000: episode: 337, duration: 7.276s, episode steps: 80, steps per second: 11, episode reward: 5.000, mean reward: 0.062 [0.000, 1.000], mean action: 2.375 [0.000, 3.000], mean observation: 39.516 [0.000, 142.000], loss: 0.194860, mean_absolute_error: 2.012368, mean_q: 2.696683, mean epsilon: 0.098 [0.098, 0.098]\n",
      "2018-07-07 23:33:59,467:atari_game:INFO: Target updated, step: 18999\n",
      "   19161/5000000: episode: 338, duration: 9.570s, episode steps: 90, steps per second: 9, episode reward: 2.000, mean reward: 0.022 [0.000, 1.000], mean action: 2.267 [0.000, 3.000], mean observation: 39.938 [0.000, 142.000], loss: 0.184511, mean_absolute_error: 1.883843, mean_q: 2.576394, mean epsilon: 0.098 [0.098, 0.098]\n",
      "   19225/5000000: episode: 339, duration: 7.329s, episode steps: 64, steps per second: 9, episode reward: 1.000, mean reward: 0.016 [0.000, 1.000], mean action: 2.078 [0.000, 3.000], mean observation: 40.001 [0.000, 142.000], loss: 0.182744, mean_absolute_error: 1.853868, mean_q: 2.562806, mean epsilon: 0.098 [0.098, 0.098]\n",
      "   19258/5000000: episode: 340, duration: 3.917s, episode steps: 33, steps per second: 8, episode reward: 1.000, mean reward: 0.030 [0.000, 1.000], mean action: 2.061 [1.000, 3.000], mean observation: 39.936 [0.000, 142.000], loss: 0.187924, mean_absolute_error: 1.821112, mean_q: 2.508682, mean epsilon: 0.098 [0.098, 0.098]\n",
      "   19325/5000000: episode: 341, duration: 15.191s, episode steps: 67, steps per second: 4, episode reward: 3.000, mean reward: 0.045 [0.000, 1.000], mean action: 2.239 [1.000, 3.000], mean observation: 39.685 [0.000, 142.000], loss: 0.189578, mean_absolute_error: 1.845492, mean_q: 2.542316, mean epsilon: 0.098 [0.098, 0.098]\n"
     ]
    }
   ],
   "source": [
    "game.main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-06-15 17:43:30,700:atari_game:INFO: Plot of Rewards was saved.\n",
      "2018-06-15 17:43:30,772:atari_game:INFO: Plot of Losses was saved.\n",
      "2018-06-15 17:43:30,860:atari_game:INFO: Plot of Episode Lengths was saved.\n"
     ]
    }
   ],
   "source": [
    "game.plot_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lived for 319 frames, total reward was 2.0\n"
     ]
    }
   ],
   "source": [
    "game.simulate_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "with open(\"/home/mholub/q_vals.txt\", \"r\") as f:\n",
    "    q_vals = []\n",
    "    for line in f:\n",
    "        q_vals.append(float(line.rstrip()))\n",
    "plt.close()\n",
    "plt.plot(q_vals)\n",
    "plt.savefig(\"/home/mholub/q_vals.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(80, 80, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "img = plt.imread(\"./anim/example.jpg\")\n",
    "img_crop = img[34:194,0:160, :]\n",
    "plt.imshow(img_crop); plt.show()\n",
    "img_crop[::2,::2].shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
